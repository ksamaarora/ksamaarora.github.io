---
layout: post
title:  "Designing and Implementing Data Science Solution on Azure (DP-100)"
date:   2024-05-08 09:29:20 +0700
tags:
  - cloud
categories: jekyll update
usemathjax: true
---

> ### <a href="/posts_blogs/blogs_dp100/datasciencelifecycle" style="color:skyblue;" rel="noopener">Data Science Life Cycle & Responsible AI Guidelines</a>

> ### Azure Machine Learning Model 

[![Screenshot-2024-05-12-at-3-53-44-PM.png](https://i.postimg.cc/mkC3Mbcq/Screenshot-2024-05-12-at-3-53-44-PM.png)](https://postimg.cc/p5VnMMJY)

> ### Resources and Assets 

<!-- [![Screenshot-2024-05-12-at-5-50-16-PM.png](https://i.postimg.cc/N0DZPCqJ/Screenshot-2024-05-12-at-5-50-16-PM.png)](https://postimg.cc/7CCB5VL0) -->

**Resources** provide the **infrastructure and services** to build solutions, and **assets** are the **products or content created or configured** by data scientists and data engineers.
Examples of resources include compute and datastore, and examples of assets are machine learning models and data assets.

> ### Designing a Data Ingestion Solution

- **Serving Data to Azure ML Workflows**:
  - **Azure Blob Storage**: Ideal for unstructured data like images, text, and JSON files.
  - **Azure Data Lake Storage (Gen2)**: Best for storing large volumes of unstructured data.
  - **Azure SQL Database**: Suitable for structured data that doesnâ€™t change over time.

- **Ingesting, Transforming, and Storing Data**:
  - **Azure Synapse Analytics**: Use for creating and scheduling data pipelines with a UI or JSON.
  - **Azure Databricks**: Opt for this if you prefer a code-first approach with SQL or Python.
  - **Azure ML**: Utilize to manage all tasks within a single tool or run pipelines on-demand with a compute cluster.

> ### Creating an Azure ML Workspace

[![Screenshot-2024-05-15-at-3-43-17-AM.png](https://i.postimg.cc/xdx59S0d/Screenshot-2024-05-15-at-3-43-17-AM.png)](https://postimg.cc/d7Cr9pZM)

[![Screenshot-2024-05-15-at-4-08-43-AM.png](https://i.postimg.cc/4NCC9yMV/Screenshot-2024-05-15-at-4-08-43-AM.png)](https://postimg.cc/vDvSRYGZ)

- **Git:** Azure ML Workspaces work indirectly with Git for **source control**, and you can use a local Git repository by cloning it
  - ```python
  git clone url_of_repository
  ```
- **Environments:** Can be another form of **source control** and multiple custom environments can be created or curated environments can be used.
    
[![Screenshot-2024-05-15-at-4-11-56-AM.png](https://i.postimg.cc/QxNQSkR3/Screenshot-2024-05-15-at-4-11-56-AM.png)](https://postimg.cc/14TV36mC)

- **Application Insights, Azure Key Vault, and Azure Storage account** are resources are **provisioned by default** when you **deploy** an Azure Machine Learning **workspace**. 

  - **Application Insights** is an extension of Azure Monitor and provides performance monitoring for several types of applications, including machine learning solutions. 
  - The **key vault** is used to store secrets required to connect to data and other resources and assets. 
  - The **storage account** is used to store machine learning notebooks and logs related to machine learning jobs and other activities.


- A **compute target** is a designated **compute resource** or **environment** where you run your training script or host your service deployment. You **can use** a **different compute target** for each phase of your project. For example, you might use a compute instance to train your model and Kubernetes for a deployed model. Compute targets can be defined for **training as well as production phases** of your Azure Machine Learning projects.

> ### Introducing Essential of Python SDK

The basic pattern used in managing Azure Machine Learning resources using the Azure Machine Learning SDK for Python starts with **importing any required libraries and classes**, then **defining the attributes and configuration** of the resource, and then executing a **create or update method**.

- **Authenticate ML Client**: From the Azure Machine Learning SDK for Python, MLClient is **used to connect to the workspace**. It is also used to execute methods that create or update resources, such as datastores and compute.
```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
```

- **Create New Machine Learning Workspace**
```python
from azureml.core import Workspace
my_ws = Workspace(
    name="another-workspace",
    location="westus",
    display_name="another-workspace for you to view", 
    description="This is just an example"
)
ml_client.workspaces.begin_create(my_ws)
```

- **Create a new Compute Targets**: Compute Targets are for production workloads and Compute Instances are used for powering notebooks 
```python
from azureml.core import ComputeTarget, AmlCompute
my_little_cluster = AmlCompute(
    name="basic-cluster",
    type="amlcompute",
    size="STANDARD_DS3_v2",
    location="westus",
    min_instances=0,
    max_instances=2,
    idle_time_before_scale_down=60,
)
ml_client.begin_create_or_update (my_little_cluster)
```

- **Create a new Datastore**
```python
from azureml.core import Datastore, AzureBlobDatastore 
a_blob_store= AzureBlobDatastore(
    name="my_blob_example",
    description="Datastore pointing to a blob container.",
    account_name="myexistingblobstore", 
    container_name="stuffandsuch-container",
    credentials={
        "account_key": "XXXXYYYXXXzzzYYYZZZxxx"
    },
)
ml_client.create_or_update(a_blob_store)
```

- **Create List Environments**
```python
envs = ml_client.environments.list()
for env in envs:
    print(env.name)
```

- **Create a Custom Environment** 
```python
from azureml.core import Environment
my_env = Environment(
    image="pytorch/pytorch: latest", # base image to use
    name="docker-image-example", # name of the model
    description="Environment created from a Docker image.",
)
ml_client.environments.create_or_update(my_env) # use the MLClient to connect to workspace and create/register the environment
```

### (I) _<u>DESIGN AND PREPARE A MACHINE LEARNING SOLUTION</u>_

> ### Compute

| **Compute Type** | **Description** |
|---|---|
| **Compute Instances** | The engine that powers your machine learning experiments. Used for everyday starting builds. Just like a Virtual Machine. |
| **Compute Clusters** | Think of compute clusters as a bunch of workers. We can scale our work using compute clusters as we can increase the number of nodes (workers) used for a particular task/ workload. Used for large runs. |
| | - **Dedicated Cluster:** These are just ready when you are - more expensive |
| | - **Low-Priority Cluster:** These systems are going to be ready when you are, probably. Low priority clusters may be accessed by multiple users thus you may get access to it but just in a few minutes or so - less expensive |
| **Inference Cluster/Kubernetes Cluster** | We will be using Azure Kubernetes or AksCompute Cluster. This will be useful for Production environments, Multi-cloud environments, On-premises |
| **Attached Compute** | "Catch-All". Used for Specialized needs. This is useful for HDInsight cluster, Virtual Machine, Databricks cluster, Data Lake Analytics, Azure Synapse Spark Pool |

> ### Apache Spark Tools as Compute Targets

**Azure Synapse:** It is an enterprise analytics service platform that enables data warehousing, analytics, processing and integration and pipeline framed with a massively parallel processing architecture. Synapse supports bot SQL and Spark technologies. 

**Azure Synapse Spark Pools:**

- When setting up a Synapse Spark pool as an attached compute target in Azure Machine Learning studio:
Select an existing Azure Synapse workspace and an existing Spark pool in that workspace -> tick the option to set up a **managed identity** -> Choose system-assigned or user-assigned -> To **reliably connect to your new compute resource** to run workloads, Go to synapse studio and **assign managed identity in Azure ML to role of Synapse Administrator**. 

- **Serverless Spark Pools** can be used as a form of compute to set up notebooks in Azure ML Studio

> ### Create Compute Targets for Experiments and Training:

Azure ML compute instance or compute clusters can be created from:
  - **Azure ML Studio**
  - **Python SDK**
  - **Azure CLI**
  - **Azure Resource Manager Templates** (You can re-use compute from the ARM templates)

**Using Python SDK:**
  - Create **Compute instance**

```python
# Compute Instances need to have a unique name across the region.
# Here we create a unique name with current datetime
from azure.ai.ml.entities import ComputeInstance, AmlCompute

ci_basic_name = "basic-ci"
ci_basic = ComputeInstance(name=ci_basic_name, size="STANDARD_DS3_v2")
ml_client.begin_create_or_update(ci_basic)
```
  - Create **Compute cluster:** Specify **size and min max instances** required. This means when the VM is **idle** it ill resize to **min instances** and whenever in use it will scale up to max instances. 

```python
from azure.ai.ml.entities import AmlCompute

cluster_basic = AmlCompute(
name="basic-example",
type="amlcompute",
size="STANDARD_DS3_v2",
location="westus",
min_instances=0,
max_instances=2,
idle_time_before_scale_down=120,
)
ml_client.begin_create_or_update(cluster_basic)
```

> ### Configure Attached Compute Resources

[![Screenshot-2024-05-15-at-3-42-05-PM.png](https://i.postimg.cc/x1vy5nFd/Screenshot-2024-05-15-at-3-42-05-PM.png)](https://postimg.cc/bZvtNcPX)


> ### Selecting development approaches to build or train models

Three ways to interact with Azure Machine Learning
  - **Azure CLI:** Use this command-line approach for automation of infrastructure. Following command is used to define an compute resource and then create a Azure ML Workspace
    - ```python
      az ml compute create -f create-cluster.yml
      ```
  - **Python SDK:** Used to submit jobs and manage models from a Jupyter notebook, ideal for data scientists - **code-first solutions**
  - **Azure ML Studio:** Use the user-friendly UI to explore workspace and other capabilities - **low code** training and development, data management and monitoring can be applied **(automated ML & visual designer)**

[![Screenshot-2024-05-13-at-3-31-18-AM.png](https://i.postimg.cc/2SHqc7ZD/Screenshot-2024-05-13-at-3-31-18-AM.png)](https://postimg.cc/hXdPhxsy)

> #### Azure Machine Learning Environments

**Environment:** Contains python packages, environment variables, software settings, runtimes. Environment ensures version control, reproducible and auditable. 

- **Environment** are divided into **3 major categories:**
  - **Curated Environment:** These are available in the workspace by default and are intended to be used as it is. These environements are **created and updated by Microsoft**.
  - **User-Managed Environment:** **You are responsible** to set up the environement as install packages required for training the script. You are responsible for everything. 
  - **System Managed:** In this, **Conda package manager** will **manage** the python environment for you.

> ### Attached Compute - HD Insights and Apache Spark

```python
from azureml.core import RemoteCompute, ComputeTarget
# Ubuntu VMs only
# VM must have public IP addy
my_resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
my_compute_target_name = "attached_existingVM"
attached_target_config = RemoteCompute.attach
#
attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',
                                                    ssh_port=22,
                                                    username= '<username>'
                                                    password="<password>")
# Attach the compute
compute = ComputeTarget.attach(my_ws, my_compute_target_name, attached_target_config)
compute.wait_for_completion (show_output=True)
```

**Note:** When considering attaching an existing virtual machine to your Azure ML workspace as a compute target, it's crucial that the **external Vms** must be **Ubuntu only** and must have an **public IP address only**.  
However, the primary reason for **choosing an existing VM** over a new compute instance is to **utilize unused capacity effectively**.

```python
from azureml.core import ScriptRunConfig
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
my_env = Environment(name="mycoenv")

# Dependencies
my_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])

# Base Image. Leave out to use the default image: "azureml.core.runconfig.DEFAULT_CPU_IMAGE"

# Configure with the existing VM as the compute target, using the environment we just defined 
src = ScriptRunConfig(source_directory=".", script="train.py", compute_target=compute, environment=my_env)
```

> ### <a href="/posts_blogs/blogs_dp100/Lab1CreateMLWorkspace" style="color:skyblue;" rel="noopener">Create Azure Machine Learning Workspace - Lab 1</a>

### (II) _<u>EXPLORE DATA AND TRAIN MODEL</u>_

> ### Managing and Exploring Data Assets 

- **Datastores:** Datastores are **reference** to existing **Azure Storage resource** and are used to **read data directly** from that source
[![Screenshot-2024-05-15-at-4-15-45-AM.png](https://i.postimg.cc/02rLFmwT/Screenshot-2024-05-15-at-4-15-45-AM.png)](https://postimg.cc/w78fs1F2)
- **Datasets:** Datasets are also **references** but are **further abstracted** to point to **specific versioned** sets of data.
- **Data Asset Management:** It is the implementation and monitoring of datastores and datasets. It is version and tracking. It is registering and retrieveing those versions. It is how we are monitoring datasets and how we look at drift detection. We can access datasets that Azure provides and look at public datasets
- **Valid asset types** can be **folders, files** or **ml table**

> **Create Data Assets Using the SDK:**
[![Screenshot-2024-05-14-at-2-45-42-AM.png](https://i.postimg.cc/1Xkc4wC7/Screenshot-2024-05-14-at-2-45-42-AM.png)](https://postimg.cc/wRksGydD)

**Note:** When working with the SDK, **import** the requires modules, followed by **defining 'my_path'** which indicates the **location of the datastore** (e.g., blob storage account). Next, **define 'my_data'** to specify the **location of the file or table** within the datastore. Finally, utilize the **'create' or 'update' function** to manipulate 'my_data' accordingly.

> ### Mounting and Downloading Files for Datasets 

|                     | Mount Files                                         | Download Files                                        |
|---------------------|-----------------------------------------------------|-------------------------------------------------------|
| **Description**         | Files do not reside in compute                     | Files downloaded to compute                           |
| **Processing**          | More streaming - means more processing/moving of data| Less streaming - less processing as data has been downloaded |
| **Usage**               | Good if you donâ€™t use all files from dataset       | Good if you use all files from dataset               |
| **Available for**       | Datasets created from ADLS, SQL, Database, PostgreSQL| Datasets created from ADLS, SQL, Database, PostgreSQL|

> **Managing Data Assets Using the SDK:**
[![Screenshot-2024-05-14-at-3-02-55-AM.png](https://i.postimg.cc/rpp9zVy4/Screenshot-2024-05-14-at-3-02-55-AM.png)](https://postimg.cc/18LFjZqR)

**In Azure ML Studio:**

- Create Datastore
```python
from azureml.core import Workspace, Datastore
ws = Workspace.get(name='DP100Testing', subscription_id='5fb9753f-1afe-4b8e-a65c-e402ecd9f0a5', resource_group='AZ800')
blob_ds = Datastore.register_azure_blob_container(
    workspace = WS, datastore_name='example',
    container_name='azureml',
    account_name='dp100testing1952224325',
    account_key='lxt3bpnDPOj zcN1081Pg38wMXI0QzRYlbbZnvghwGd82A518uj 5NTZGWZr7F/120jZvVuVGQ4XSk+ASt91vKog=='
)
```

- Print Datastore 
```python
for ds_name in ws.datastores:
    print(ds_name)
```

- Create dataset
```python
VERSION = "1"
my_data = Data(
    path = https://dp100testing1952224325.blob.core.windows.net/azureml/AdventureWorks Sales-2.xlsx,
    type = AssetTypes.URI_FILE,
    description = "Sales",
    name = "AW-Sales",
    version = VERSION
)
ml_client.data.create_or_update(my_data)
```

> ### Preprocessing of Data 

**Steps** for Preprocessing of Data:
- **Data quality assessment**
- **Data Cleaning:** Look for Missing data, Noisy data
- **Data transformation:** Aggregation, Feature selection, Normalization (combining all the data so that everything is standardized)
- **Data reduction:** employ math to filter out unnecessary data

> ### Feature Selection and Feature Engineering

- **Feature Selection:** The process of **selecting specific variables (features)** that contribute the most to the prediction variable in our algorithms.

- **Feature Engineering:** The process of **selecting and expressing data** in a way that improves the performance of machine learning models. 
    - **Wrapper method:** Wrapper methods **evaluate subsets of features** by training a model on them and assessing the model's performance. All **combinations** are evaluated and the **best one** is chosen. Prone to overfitting
    - **Filter method:** Filter methods **apply statistical techniques** to evaluate the relevance of each feature independently from the machine learning model. They **assign a score to each feature** based on various statistical tests and **rank them accordingly**. Thus **Features** can then be **included or removed** from the dataset **based on their scores**. This method is **relationship based**. Faster than wrapper method. 

> ### Differential Privacy: Eg of Responsible AI

**Differential Privacy** seeks to **protect individual data by adding statistical noise to the analysis process**. **Minimizing** risk of **personal identification** and **ensuring data privacy.** It ensures that the **output** of a data analysis algorithm does **not reveal sensitive information** about any individual.
- When applying differential privacy, **Epsilon (Îµ), Bounds and Sample size** needs to be **defined**
- **Privacy Loss Paramter/Epsilon (Îµ)** is a key parameter in differential privacy that **controls the balance between privacy and accuracy - Value ranges between 0 and 1**
  - **Low Epsilon (Îµ)**: **High privacy** - **Low accuracy** - More noise - High data obscurity - Data more difficult to interpret
  - **High Epsilon (Îµ)**: **Low Privacy** - **High accuracy** - Less noise - Less data obscurity - Data more accurate and easier to interpret 

```python
# Sample code snippet
privacy_usage = { 'epsilon': 0.10},
                data_lower = lower_range[1],
                data_upper = upper_range[10],
                data_rows = sample
```

> ### Accessing Data During Interactive Development

**Data Wrangling:** It is the process of **transforming data** to a format thatâ€™s best suited to the needs of ML model. 

- Step 1: Open a notebook with a running ML Azure Kernel

- Step 2: Create **ML Client** and a **datastore** (code mentioned before)

- Step 3: Build a **URI** (**Uniform Resource Identifier**: It specifies location of a resource) OR **Directly grab** it in the **Studio UI**

```python
# Azure Machine Learning workspace details:
subscription = '<subscription_id>'
resource_group = '<resource_group>'
workspace = '<workspace>'
datastore_name = '<datastore>'
path_on_datastore = '<path>'

# Long-form Datastore URI format:
uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}'
```

OR go to Data -> specific Datastore -> select csv file -> copy URI (Datastore URI or Storage URI)
  - **Note:** The **Datastore URI** is only **applicable to Azure ML** and the **Storage URI** is a more generic storage endpoint, which is used only **outside Azure ML**.

- Step 4: Load a **Pandas Dataframe**

```python
# Import pandas library
import pandas as pd

# Populate dataframe "my_dataframe" using the pandas read CSV method by passing in the URI acquired
my_dataframe = pd.read_csv("URI")

# Then run dataframe head passing in a value for the number of rows you want to return
my_dataframe.head(1000)
```

- Step 5: **Wrangle** - Replace **Missing** Strings

```python
# Fill missing values in the "Claim Network Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Claim Network Status": "Unkown"}, inplace=True)

# Fill missing values in the "Payment Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Payment Status": "Unkown"}, inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 6: **Wrangle** - **Delete Rows** With any **Empty** Columns

```python
# Drop rows with any missing values and update the dataframe in place
my_dataframe.dropna(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

> ### Wrangling Data with Apache Spark

- Step 1: Create a **compute** to power the notebook - Compute instance / Synapse Spark Pool / Azure ML Serverless Spark

- Step 2: Open a **notebook** and we use **Azure ML Serverless Spark** as compute for ease of use

- Step 3: Build a **URI** or grab it from the Studio UI

- Step 4: Load a **PySpark Pandas Dataframe**

```python
# Import pyspark pandas library
import pyspark.pandas as pd
my_dataframe = pd.read_csv("URI")
my_dataframe.head(1000)
```

- Step 5: Wrangle - Replace **Missing** Strings (code given above)

- Step 6: Wrangle - **Delete rows** with any **empty** columns (code given above)

- Step 7: **Wrangle** - Remove **Duplicate** Rows 

```python
# Drop duplicate rows and update the dataframe in place
my_dataframe.drop_duplicates(inplace=True)

# Sort the dataframe by its index and update the dataframe in place
my_dataframe.sort_index(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 8: **Save** the Transformed Data

```python
# Save the dataframe to a CSV file at the specified Azure Data Lake Storage path
my_dataframe.to_csv(
    "abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled"
    )
```

> ### <a href="/posts_blogs/blogs_dp100/Lab2WrangleData" style="color:skyblue;" rel="noopener">Wrangle Data with Python in Azure ML - Lab 2</a>

### (III) _<u>THREE WAYS TO BUILD AND TRAIN ML MODEL</u>_

> ### Creating Models in the Azure ML Designer 

- Step 1: Open **Azure ML Studio** -> Select **Designer**
- Step 2: Create new **pipeline (Classic Pre-built/Custom)**. Say we select a classic pre-built model for example - Regression Automobile Price prediction (Basic)
- Step 3: Now **run** this model by setting up a **pipeline job and a compute**
- Step 4: **Evaluate** the model based on metrics: **coefficient of determination, mean absolute error, relative absolute error, relative squared error, root mean squared error**

> **Dataset -> Select Columns in Dataset -> Clean Missing Data -> Split Data & (Linear Regression) -> Train Model -> Score Model -> Evaluate Model**

[![Screenshot-2024-05-16-at-1-06-58-AM.png](https://i.postimg.cc/26zyrWfp/Screenshot-2024-05-16-at-1-06-58-AM.png)](https://postimg.cc/B8V4CX6N)

> ### Custom Code Components

Custom code modules are created using Python. Supported libraries include NumPy, SciPy, scikit-learn, Theano, TensorFlow, Keras, PyTorch, pandas, and matplotlib

- **Create Custom Model:** Develop your custom model using **"Create Python Model"** module
- **Link** the custom model to the **"Train Model"** module to train it with your dataset
- Since standard evaluation modules are **not compatible**, use the **"Execute Python Script" module** to run evaluation scripts for the custom model

[![Screenshot-2024-05-16-at-1-07-53-AM.png](https://i.postimg.cc/nhyTRFf9/Screenshot-2024-05-16-at-1-07-53-AM.png)](https://postimg.cc/4Y1znTdX)
[![Screenshot-2024-05-16-at-1-08-23-AM.png](https://i.postimg.cc/Kj8dGGzv/Screenshot-2024-05-16-at-1-08-23-AM.png)](https://postimg.cc/7GjBXy0p)

> ### <a href="/posts_blogs/blogs_dp100/Lab3CreateBasicPipeline" style="color:skyblue;" rel="noopener">Create a basic pipeline in Azure ML Studio - Lab 3</a>

> ### Filter Based Feature Selection Module

- **Configuration Options:**
  - Operate on Feature Columns only - True/False
  - Number of desires features (Specify the number of features to output in the results): 1
  - Feature scoring method - **PearsonCorrelation/ChiSquared**
  - Target Column: Specify target column

### Pearson Correlation vs Chi-Squared Statistics

| Aspect                      | Pearson Correlation                                  | Chi-Squared Statistics                                    |
|-----------------------------|-----------------------------------------------------|----------------------------------------------------------|
| **Purpose**                 | Measures the strength and direction of a linear **relationship** between two quantitative variables | It is a comparative test that reveals **how close expected values are to actual results**. |
| **Correlation Coefficient (R)** | Ranges from -1 to +1: <br> - **0**: No correlation <br> - **+1**: Perfect positive correlation <br> - **-1**: Perfect negative correlation | **No correlation coefficient**; lower values indicate a better fit to expected values |
| **Type of Variables**       | **Quantitative**                                        | **Categorical**                                              |
| **Interpretation**          | - **Positive Correlation**: e.g., More rain increases humidity <br> - **Negative Correlation**: e.g., Higher altitude decreases temperature | **Indicates** whether a **relationship exists** but does **not specify the type** (positive or negative) |
| **Value Indication**        | Indicates the strength of the relationship          | Smaller values indicate a better fit and existence of a relationship |
| **Steps**                   | 1. Determine linearity <br> 2. Clean data <br> 3. Generate the coefficient <br> 4. Evaluate the results |  |

> ### Permutation Feature Importance Module

It refers to **randomly shuffling data while keeping everything else constant** and then seeing if we have a change in whatever feature column we were looking at. From that we can generate new prediction based on results. Compute feature importance score by calculating decrease in quality

- **Configuration Options:**
  - **Random seed** (Random number generator seed value - It is going to **randomize the data**) = 1023 (Say)
  - **Metric for measuring performance:**
    - **Classification metrics** - **Accuracy** (how accurate model is), **Precision** (how good model is), **Recall** (how many times model was able to detect a specific category)
    - **Regression metrics** - **Mean Absolute Error**, **Root Mean Squared Error**, **Relative Absolute Error**, **Relative Squared Error**, **Coefficient of Determination** (R squared)

> ### Applying Automated ML to Explore Models

Automated ML democratizes machine learning with a **no-code approach**, making it easy to explore optimal machine learning models. Automated ML handles **preprocessing, featurization, transformation, scaling,** and **normalization**. At the end, it **scores** the model by selecting a metric and **deploying** the model. Automated ML is used for exploring optimal algorithms and parameters to **solve** a particular problem **without a lot of human trial-and-error**.

- **Types of algorithm** in automated ML: **Classification**, **Regression** and **Time Series Forecasting** 

**Examples of Automated ML**:
- Tabular Data
- Computer Vision
- Natural Language Processing

**Steps to Apply Automated ML**

- Step 1: **Create ML Client**

- Step 2: **Define MLTable**: (ML Table is already inputted)

- Step 3: **Define the AutoML Job**

```python
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import automl, Input

# Create an Input object for the training data
my_training_input = Input(
    type=AssetTypes.MLTABLE, 
    path="./data/training-mltable-folder"
)

# Configure the classification job
my_classification_job = automl.classification(
    compute="aemcmlcompute",
    experiment_name="this_experiment",
    training_data=my_training_input,
    target_column_name="try_me",
    primary_metric="accuracy",
    n_cross_validations=4,
    enable_model_explainability=True,
    tags={"my_tag": "My value"}
)

# Set optional limits
my_classification_job.set_limits(
    timeout_minutes=600,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True
)

# Set optional training properties
my_classification_job.set_training(
    blocked_training_algorithms=["logistic_regression"], 
    enable_onnx_compatible_models=True
)
```

- Step 4: **Submit (Run) the Job**

```python
# Submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    my_classification_job
)
returned_job.services ["Studio"].endpoint
```

> ### <a href="/posts_blogs/blogs_dp100/Lab4AzureAutomatedML" style="color:skyblue;" rel="noopener">Using Azure Automated Machine Learning - Lab 4</a>

> ### <a href="/posts_blogs/blogs_dp100/TrainModelUsingPythonSDK" style="color:skyblue;" rel="noopener">Training Model by using Python SDK</a>

[![Screenshot-2024-05-16-at-10-42-51-PM.png](https://i.postimg.cc/s2PhBMwp/Screenshot-2024-05-16-at-10-42-51-PM.png)](https://postimg.cc/hJjj6tFj)

> ### Tuning Hyperparameters in Azure Machine Learning

**Hyperparameters**: **Top-level settings** you configure before running the ML algorithm, such as: train-test ratios, number of epochs, batch size

Types of Hyperparameters

- **Discrete Hyperparameters**: **Specified as a choice among discrete values** - Choose from a set of explicit values (e.g., 1, 2, 3). E.g. QUniform

```python
{
    "batch_size": choice(1, 2, 3, 4)
    "number_of_hidden_layers": choice(range(1,5))
}
```

- **Continuous Hyperparameters**:  **Specified from a continuous (sliding) range of values** - Defined over a range, sampled from distributions:
  - Normal distribution
  - Uniform distribution
  - Lognormal and loguniform distributions

```python
{
    "learning_rate": normal(10,3),
    "keep_probability":uniform(0.05, 0.1)
}
```

- **Hyperparameter tuning:** Process of finding the configuration of Hyperparameters that results in the best performance 
- **SweepJob**:  helps you automate choosing these parameters

- **Sampling Methods**
[![Screenshot-2024-05-16-at-10-45-10-PM.png](https://i.postimg.cc/0yVpZMpP/Screenshot-2024-05-16-at-10-45-10-PM.png)](https://postimg.cc/RWn6Z0sD)

> ### Early Termination Policies

- **Increase efficiency** and **reduce costs** by stopping underperforming runs early.

- **_<u>Bandit Policy</u>_**: 
  - **Stops runs** that perform **worse than the best run by a specified margin**
  - **Slack amount** = 0.2 means that performance if less than 20 percent of best run, then run stops

```python
from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy
            (slack_amount = 0.2, evaluation_interval=1, delay_evaluation=5)
```

- **_<u>Truncation Selection Policy</u>_**: 
  - **Stops** the **lowest-performing runs** at each intervals.

```python
from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelection Policy
            (truncation_percentage=25, evaluation_interval=1, delay_evaluation=2)
```

- **_<u>Median Stopping Policy</u>_**: 
  - **Stops** runs that p**erform worse than the median of running averages**.

```python
from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy
            (evaluation_interval=1, delay_evaluation=2)
```

> ### <a href="/posts_blogs/blogs_dp100/OptimizeHyperparameters" style="color:skyblue;" rel="noopener">Steps to Optimize Hyperparameters</a>



 









