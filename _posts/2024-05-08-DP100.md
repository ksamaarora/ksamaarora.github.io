---
layout: post
title:  "Designing and Implementing Data Science Solution on Azure (DP-100)"
date:   2024-05-08 09:29:20 +0700
tags:
  - cloud
categories: jekyll update
usemathjax: true
---

- <a href="/posts_blogs/blogs_dp100/datasciencelifecycle" style="color:skyblue;" rel="noopener">Data Science Life Cycle & Responsible AI Guidelines</a> - Overview

> ### Designing a Data Ingestion Solution*

**Overview:** Extract **raw data** from source (CRM or IoT device) -> Copy and transform data with **Azure Synapse Analytics** -> **Store** prepared data in **Azure Blob Storage** -> **Train** model with **Azure ML**

| **Type of Data**       | **Description**                                                                                                                           | **Example**                                                                                                           |
|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Tabular or Structured Data**  | All data has the **same fields** or properties, defined in a schema. Represented in **tables** where **columns** are features and **rows** are data points. | **Excel** or **CSV** file |
| **Semi-structured Data** | **Not all data** has the **same fields** or properties. Represented by a collection of **key-value pairs**. Keys are features, and values are properties. | **JSON object from IoT** device: <br> { "deviceId": 29482, "location": "Office1", "time":"2021-07-14T12:47:39Z", "temperature": 23 } |
| **Unstructured Data**  | Files that **don't adhere to any rules** regarding **structure**. **Can't query** the data in the database. | **Document, image, audio, or video** file |

> **Store data for model training workloads:*** 

- When using Azure Machine Learning, Azure Databricks, or Azure Synapse Analytics for model training, three common options for storing data are:

  - **Azure Blob Storage:** Cheapest option for **storing unstructured data**. Ideal for storing files like images, text, CSV, and JSON 

  - **Azure Data Lake Storage (Gen 2):** Best for **storing large volumes (limitless) of unstructured data** like CSV files and images. A data lake also implements a **hierarchical namespace**, which means it’s **easier** to give **someone access** to a **specific file or folder (privacy-sensitive)**

  - **Azure SQL Database:** **Stores unstructured data** that **doesn’t change over time**. Data is read as a table and schema is defined when a table in the database is created.

> **Create a data ingestion pipeline:***

- **Azure Synapse Analytics:** 
  - Used to create and schedule data ingestion pipelines through the **easy-to-use UI**, or by defining the pipeline in JSON format.
  - UI tool like **mapping data flow** or using a language like SQL, Python, or R.
  - Creates **automated pipelines** to **copy and move** data
  - Allows to choose between different compute - serverless SQL pools, dedicated SQL pools, or Spark pools

- **Azure Databricks:** 
  - Opt for this if you prefer a **code-first tool** and to use **SQL, python or R** to create pipelines. 
  - Azure Databricks **uses PySark**, which **distribute compute** to **transform large amounts of data** in **less time**.

- **Azure Machine Learning:** 
  - Provides **compute clusters** (automatically scale up or down when needed). 
  - Used to **manage all tasks with a single tool** or run pipelines with **on-demand compute cluster**. 
  - **Python** is preffered language 
  - Can create **pipeline using Designer**. 
  - **Intutive user interface**

- **Azure AI Services:**
  - Collection of **pre-built ML models**, this saves time and effort to train model
  - Models are offered as an **API** (Application Programming Interface)

> ### Selecting development approaches to build or train models*

Three ways to interact with Azure Machine Learning
  - **Azure CLI:** Use this command-line approach for automation of infrastructure. 
  - **Python SDK:** Used to submit jobs and manage models from a Jupyter notebook, ideal for data scientists - **code-first solutions**
  - **Azure ML Studio:** Use the **user-friendly UI** to explore workspace and other capabilities - **low code** training and development, data management and monitoring can be applied **(automated ML & visual designer)**

[![Screenshot-2024-05-13-at-3-31-18-AM.png](https://i.postimg.cc/2SHqc7ZD/Screenshot-2024-05-13-at-3-31-18-AM.png)](https://postimg.cc/hXdPhxsy)

- **Git:** Azure ML Workspaces work indirectly with Git for **source control**, and you can **use** a **local** Git repository by **cloning** it (_git clone url_of_repository_)
[![Screenshot-2024-05-15-at-4-11-56-AM.png](https://i.postimg.cc/QxNQSkR3/Screenshot-2024-05-15-at-4-11-56-AM.png)](https://postimg.cc/14TV36mC)

> ### <a href="/posts_blogs/blogs_dp100/environments" style="color:skyblue;" rel="noopener">Azure Machine Learning Environments* </a> (Imp)

**Environment:** Can be another form of **source control** and multiple custom environments can be created or curated environments can be used. Contains python packages, environment variables, software settings, runtimes. Environment **ensures version control, reproducible and auditable**. 

- **Environment** are divided into **3 major categories:**
  - **Curated Environment:** These are **automatically created** when ML workspace is created **(pre-built environments)**. These environements are **created and updated by Microsoft**. Curated environments **use prefix AzureML-** - **faster development time**
  - **User-Managed Environment:** **You are responsible** to set up the environement as install packages required for training the script. You are responsible for everything. 
  - **System Managed:** In this, **Conda package manager** will **manage** the python environment for you.

> ### Azure ML Workspace

[![Screenshot-2024-05-15-at-3-43-17-AM.png](https://i.postimg.cc/xdx59S0d/Screenshot-2024-05-15-at-3-43-17-AM.png)](https://postimg.cc/d7Cr9pZM)

[![Screenshot-2024-05-15-at-4-08-43-AM.png](https://i.postimg.cc/4NCC9yMV/Screenshot-2024-05-15-at-4-08-43-AM.png)](https://postimg.cc/vDvSRYGZ)

- The following are **provisioned by default** when you **deploy** an Azure Machine Learning **workspace**. 
  - **Application Insights** is used to monitor predictive services in the workspace.
  - **Azure key vault** is used to securely manage & store secrets such as authentication keys and credentials used by the workspace. 
  - **Azure storage account** is used to store machine learning notebooks and logs related to machine learning jobs and other activities.
  - **Azure Container Registery** is created when needed to store images for Azure ML environments

- ### <a href="/posts_blogs/blogs_dp100/creatingAzureMLWorkspace" style="color:skyblue;" rel="noopener">Creating Azure ML Workspace using Python SDK and CLI</a> (Imp)

> ### Resources and Assets*

- **_<u>Azure ML Resources:</u>_** provide infrastructure and services to build solution

  > **Workspace** (explained above)

  > **Compute Resources**: 5 types - Compute Instance, Compute Clusters, Kubernetes Clusters, Attached Compute, Serverless Compute

  > **Datastores**

| Datastore                | Description                                                                               | Storage Type     | Main Usage                        |
|--------------------------|-------------------------------------------------------------------------------------------|------------------|-----------------------------------|
| **workspaceartifactstore** | Stores compute and experiment logs                                                       | Azure Blob       | Logs for jobs                     |
| **workspaceworkingdirectory** | Stores files uploaded via the Notebooks section                                         | Azure File Share | Notebook files                    |
| **workspaceblobstore**   | Default datastore for storing uploaded data                                                | Azure Blob       | Data assets                       |
| **workspacefilestore**   | General file storage                                                                       | Azure File Share | General file storage              |

  - **_Azure ML Assets:_** are product or contents created and configured by data scientists and engineers
    - Models
    - Environments
    - Data
    - Compoenents

### (I) _<u>DESIGN AND PREPARE A MACHINE LEARNING SOLUTION</u>_

> ### Compute*

- **Compute Target**: is a **compute resource/environment** used to train and host model. Can use different compute target for each phase (training/production) of project. 

| **Compute Type** | **Description** |
|---|---|
| **Compute Instances** | **Similar to a VM** - Primarily used to **run notebooks** - **Ideal for experimentation** - Easiest option to work with compute instance is through the **integrated notebooks experience** in the **Azure ML** studio. OR use **VS Code** for **easier source control** of your code. |
| **Compute Clusters** | **On-demand multi-node clusters** of **CPU or GPU** - **automatically** scale - used for **large volume** of **data** - allow for **parallel processing** to distribute workload - reduce time of run. - Ideal to use for **production workloads** |
| | - **Dedicated Cluster:** These are **just ready** when you are - more expensive |
| | - **Low-Priority Cluster:** These systems are **going to be ready** when you are, probably. Low priority clusters may be accessed by multiple users thus you may get access to it but just in a few minutes or so - less expensive |
| **Inference Cluster/Kubernetes Cluster** | Allows you to create or attach an **Azure Kubernetes Service (AKS) cluster**. Ideal to deploy trained machine learning models in production scenarios. |
| **Attached Compute** | **Allows** you **to attach other** Azure compute resources to the workspace, like Azure Databricks, HDInsight cluster or Synapse Spark pools. Used for Specialized needs. |
| **Serverless Compute** | A **fully managed, on-demand compute** you can use **for training jobs**. |

- **_Central Processing Unit (CPU) or Graphical Processing Unit (GPU)_** *
  - **CPU** - sufficient and **cheaper** to use for **smaller tabular datasets**
  - **GPU** - powerful and effective for **unstructured data** - for **larger** amount of tabular data - libraries such has **RAPIDs (developed by NVIDIA)** allow data prep and training with large datasets
  
- **_General purpose or memory optimized_** *
  - When creating compute resources, there are 2 VM sizes you choose from  
    - **General purpose:** Have a **balanced CPU-to-memory ratio**. Ideal for testing and development with **smaller datasets**.
    - **Memory optimized:** Have a **high memory-to-CPU ratio**. Great for **in-memory analytics**, which is ideal when you have **larger datasets** or when you're **working in notebooks**.

- **_Spark Compute / Clusters_** *
  - Spark cluster consists of **driver node** and **worker nodes**. Code will **initially** communicate with **driver** node. The work is then **distributed across the worker** nodes. This **reduces processing time**. Finally work is summarized and the driver node communicates the result back to you. 
  - Code needs to be written in **Spar-friendly language** like **Scala, SQK, Rspark, or PySpark** in order to distribute the workload. 

> ### Apache Spark Tools as Compute Targets

**Azure Synapse:** It is an **enterprise analytics service platform** that enables data warehousing, analytics, processing and integration and pipeline framed with a **massively parallel processing** architecture. **Synapse supports** bot **SQL and Spark** technologies. 

**Azure Synapse Spark Pools:**

- When setting up a Synapse Spark pool as an attached compute target in Azure Machine Learning studio:
Select an existing Azure Synapse workspace and an existing Spark pool in that workspace -> tick the option to set up a **managed identity** -> Choose system-assigned or user-assigned -> To **reliably connect to your new compute resource** to run workloads, Go to synapse studio and **assign managed identity in Azure ML to role of Synapse Administrator**. 

**Serverless Spark Pools** can be used as a **form of compute to set up notebooks** in Azure ML Studio

> ### Create Compute Targets for Experiments and Training:*

Azure ML compute instance or compute clusters can be created from:
  - **Azure ML Studio**
  - **Python SDK**
  - **Azure CLI**
  - **Azure Resource Manager Templates** (can re-use compute from the ARM templates)

### <a href="/posts_blogs/blogs_dp100/creatingComputeTarget" style="color:skyblue;" rel="noopener">Create Compute Target using Python SDK and CLI</a> (Imp)

> ### Configure Attached Compute Resources*

[![Screenshot-2024-05-15-at-3-42-05-PM.png](https://i.postimg.cc/x1vy5nFd/Screenshot-2024-05-15-at-3-42-05-PM.png)](https://postimg.cc/bZvtNcPX)

> ### Attached Compute - HD Insights and Apache Spark

```python
from azureml.core import RemoteCompute, ComputeTarget
# Ubuntu VMs only
# VM must have public IP addy
my_resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
my_compute_target_name = "attached_existingVM"
attached_target_config = RemoteCompute.attach
#
attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',
                                                    ssh_port=22,
                                                    username= '<username>'
                                                    password="<password>")
# Attach the compute
compute = ComputeTarget.attach(my_ws, my_compute_target_name, attached_target_config)
compute.wait_for_completion (show_output=True)
```

**Note:** When considering attaching an existing virtual machine to your Azure ML workspace as a compute target, it's crucial that the **external Vms** must be **Ubuntu only** and must have an **public IP address only**.  
However, the primary reason for **choosing an existing VM** over a new compute instance is to **utilize unused capacity effectively**.

```python
from azureml.core import ScriptRunConfig
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
my_env = Environment(name="mycoenv")

# Dependencies
my_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])

# Base Image. Leave out to use the default image: "azureml.core.runconfig.DEFAULT_CPU_IMAGE"

# Configure with the existing VM as the compute target, using the environment we just defined 
src = ScriptRunConfig(source_directory=".", script="train.py", compute_target=compute, environment=my_env)
```

> ### <a href="/posts_blogs/blogs_dp100/Lab1CreateMLWorkspace" style="color:skyblue;" rel="noopener">Create Azure Machine Learning Workspace - Lab 1</a>

### (II) _<u>EXPLORE DATA AND TRAIN MODEL</u>_

> **Terminologies**:*

> **URI (Uniform Resource Identifier)**:* 

  - It specifies **location of data/resource**. 
  - To connect Azure ML to your data, a **protocol needs to be prefixed to the URI**. 
  - The **three common protocols** are:
    - **_http(s):_** Use for data stores **publicly or privately** in an **Azure Blob Storage** or **publicly available http(s)** location. (Protocol used when accessing data stored in publicly available http(s) location)
    - **_abfs(s):_** Use for data stores in an **Azure Data Lake Storage Gen 2**.
    - **_azureml:_** Use for **data stored** in a **datastore**.

> **Datastores**:* 

  - Datastores are **reference** to existing **Azure Storage resource** and are used to **read data directly** from that source.
  - **Benefits:**
    - Provides **easy-to-use URI** to your data storage
    - Facilitates **data discovery** within Azure ML
    - **Securely stores connection** information, **without exposing secrets and keys** to data scientists 
  - **Two methods for authentication:**
    - **Credential Based:** Use a service principal, **shared access signature (SAS) token** or **account key** to authenticate access to your storage account
    - **Identity-Based:** Use your **Microsoft Entra identity** or **managed identity**
  - **Types of datastores:**
  [![Screenshot-2024-05-15-at-4-15-45-AM.png](https://i.postimg.cc/02rLFmwT/Screenshot-2024-05-15-at-4-15-45-AM.png)](https://postimg.cc/w78fs1F2)

> **Data Assets:*** 

- They are **references to data in datastores**
- **Benefits**:
  - Can **share** and **reuse** data 
  - Can seamlessly access data during model training or **any supported compute** type
  - Can **version metadata** of the data asset
- **Three types of data assets:**
  - **URI file:** Points to specific file
  - **URI folder:** Points to specific folder
  - **ML Table:** Points to folder or file, including a schema to read as tabular data

> **Data Asset Management:** It is the **implementation** and **monitoring** of datastores and datasets. It is **version and tracking**. It is registering and **retrieveing those versions**. It is how we are monitoring datasets and how we look at **drift detection**. We can access datasets that Azure provides and look at public datasets

### <a href="/posts_blogs/blogs_dp100/createDatastoresDataAssets" style="color:skyblue;" rel="noopener">Create Datastores and Data Assets using Python SDK</a> (Imp)

> ### Mounting and Downloading Files for Datasets 

|                     | Mount Files                                         | Download Files                                        |
|---------------------|-----------------------------------------------------|-------------------------------------------------------|
| **Description**         | Files do not reside in compute                     | Files downloaded to compute                           |
| **Processing**          | More streaming - means more processing/moving of data| Less streaming - less processing as data has been downloaded |
| **Usage**               | Good if you don’t use all files from dataset       | Good if you use all files from dataset               |
| **Available for**       | Datasets created from ADLS, SQL, Database, PostgreSQL| Datasets created from ADLS, SQL, Database, PostgreSQL|

**In Azure ML Studio:**

- Create Datastore
```python
from azureml.core import Workspace, Datastore
ws = Workspace.get(name='DP100Testing', subscription_id='5fb9753f-1afe-4b8e-a65c-e402ecd9f0a5', resource_group='AZ800')
blob_ds = Datastore.register_azure_blob_container(
    workspace = WS, datastore_name='example',
    container_name='azureml',
    account_name='dp100testing1952224325',
    account_key='lxt3bpnDPOj zcN1081Pg38wMXI0QzRYlbbZnvghwGd82A518uj 5NTZGWZr7F/120jZvVuVGQ4XSk+ASt91vKog=='
)
```

- Print Datastore 
```python
for ds_name in ws.datastores:
    print(ds_name)
```

- Create dataset
```python
VERSION = "1"
my_data = Data(
    path = https://dp100testing1952224325.blob.core.windows.net/azureml/AdventureWorks Sales-2.xlsx,
    type = AssetTypes.URI_FILE,
    description = "Sales",
    name = "AW-Sales",
    version = VERSION
)
ml_client.data.create_or_update(my_data)
```

> ### Preprocessing of Data 

**Steps** for Preprocessing of Data:
- **Data quality assessment**
- **Data Cleaning:** Look for Missing data, Noisy data
- **Data transformation:** Aggregation, Feature selection, Normalization (combining all the data so that everything is standardized)
- **Data reduction:** employ math to filter out unnecessary data

> ### Feature Selection and Feature Engineering

- **Feature Selection:** The process of **selecting specific variables (features)** that contribute the most to the prediction variable in our algorithms.

- **Feature Engineering:** The process of **selecting and expressing data** in a way that improves the performance of machine learning models. 
    - **Wrapper method:** Wrapper methods **evaluate subsets of features** by training a model on them and assessing the model's performance. All **combinations** are evaluated and the **best one** is chosen. Prone to overfitting
    - **Filter method:** Filter methods **apply statistical techniques** to evaluate the relevance of each feature independently from the machine learning model. They **assign a score to each feature** based on various statistical tests and **rank them accordingly**. Thus **Features** can then be **included or removed** from the dataset **based on their scores**. This method is **relationship based**. Faster than wrapper method. 

> ### Differential Privacy: Eg of Responsible AI*

**Differential Privacy** seeks to **protect individual data by adding statistical noise to the analysis process**. **Minimizing** risk of **personal identification** and **ensuring data privacy.** It ensures that the **output** of a data analysis algorithm does **not reveal sensitive information** about any individual.
- When applying differential privacy, **Epsilon (ε), Bounds and Sample size** needs to be **defined**
- **Privacy Loss Paramter/Epsilon (ε)** is a key parameter in differential privacy that **controls the balance between privacy and accuracy - Value ranges between 0 and 1**
  - **Low Epsilon (ε)**: **High privacy** - **Low accuracy** - More noise - High data obscurity - Data more difficult to interpret
  - **High Epsilon (ε)**: **Low Privacy** - **High accuracy** - Less noise - Less data obscurity - Data more accurate and easier to interpret 

```python
# Sample code snippet
privacy_usage = { 'epsilon': 0.10},
                data_lower = lower_range[1],
                data_upper = upper_range[10],
                data_rows = sample
```

> ### Accessing Data During Interactive Development

**Data Wrangling:** It is the process of **transforming data** to a format that’s best suited to the needs of ML model. 

- Step 1: Open a notebook with a running ML Azure Kernel

- Step 2: Create **ML Client** and a **datastore** (code mentioned before)

- Step 3: Build a **URI** OR **Directly grab** it in the **Studio UI**

```python
# Azure Machine Learning workspace details:
subscription = '<subscription_id>'
resource_group = '<resource_group>'
workspace = '<workspace>'
datastore_name = '<datastore>'
path_on_datastore = '<path>'

# Long-form Datastore URI format:
uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}'
```

OR go to Data -> specific Datastore -> select csv file -> copy URI (Datastore URI or Storage URI)
  - **Note:** The **Datastore URI** is only **applicable to Azure ML** and the **Storage URI** is a more generic storage endpoint, which is used only **outside Azure ML**.

- Step 4: Load a **Pandas Dataframe**

```python
# Import pandas library
import pandas as pd

# Populate dataframe "my_dataframe" using the pandas read CSV method by passing in the URI acquired
my_dataframe = pd.read_csv("URI")

# Then run dataframe head passing in a value for the number of rows you want to return
my_dataframe.head(1000)
```

- Step 5: **Wrangle** - Replace **Missing** Strings

```python
# Fill missing values in the "Claim Network Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Claim Network Status": "Unkown"}, inplace=True)

# Fill missing values in the "Payment Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Payment Status": "Unkown"}, inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 6: **Wrangle** - **Delete Rows** With any **Empty** Columns

```python
# Drop rows with any missing values and update the dataframe in place
my_dataframe.dropna(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

> ### Wrangling Data with Apache Spark

- Step 1: Create a **compute** to power the notebook - Compute instance / Synapse Spark Pool / Azure ML Serverless Spark

- Step 2: Open a **notebook** and we use **Azure ML Serverless Spark** as compute for ease of use

- Step 3: Build a **URI** or grab it from the Studio UI

- Step 4: Load a **PySpark Pandas Dataframe**

```python
# Import pyspark pandas library
import pyspark.pandas as pd
my_dataframe = pd.read_csv("URI")
my_dataframe.head(1000)
```

- Step 5: Wrangle - Replace **Missing** Strings (code given above)

- Step 6: Wrangle - **Delete rows** with any **empty** columns (code given above)

- Step 7: **Wrangle** - Remove **Duplicate** Rows 

```python
# Drop duplicate rows and update the dataframe in place
my_dataframe.drop_duplicates(inplace=True)

# Sort the dataframe by its index and update the dataframe in place
my_dataframe.sort_index(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 8: **Save** the Transformed Data

```python
# Save the dataframe to a CSV file at the specified Azure Data Lake Storage path
my_dataframe.to_csv(
    "abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled"
    )
```

> ### <a href="/posts_blogs/blogs_dp100/Lab2WrangleData" style="color:skyblue;" rel="noopener">Wrangle Data with Python in Azure ML - Lab 2</a>

### (III) _<u>THREE WAYS TO BUILD AND TRAIN ML MODEL</u>_

> ### (A) Build & Train Models using <u>Azure ML Designer</u> 

- Step 1: Open **Azure ML Studio** -> Select **Designer**
- Step 2: Create new **pipeline (Classic Pre-built/Custom)**. Say we select a classic pre-built model for example - Regression Automobile Price prediction (Basic)
- Step 3: Now **run** this model by setting up a **pipeline job and a compute**
- Step 4: **Evaluate** the model based on metrics: **coefficient of determination, mean absolute error, relative absolute error, relative squared error, root mean squared error**

> **Dataset -> Select Columns in Dataset -> Clean Missing Data -> Split Data & (Linear Regression) -> Train Model -> Score Model -> Evaluate Model**

[![Screenshot-2024-05-16-at-1-06-58-AM.png](https://i.postimg.cc/26zyrWfp/Screenshot-2024-05-16-at-1-06-58-AM.png)](https://postimg.cc/B8V4CX6N)

> ### Custom Code Components

Custom code modules are created using Python. Supported libraries include NumPy, SciPy, scikit-learn, Theano, TensorFlow, Keras, PyTorch, pandas, and matplotlib

- **Create Custom Model:** Develop your custom model using **"Create Python Model"** module
- **Link** the custom model to the **"Train Model"** module to train it with your dataset
- Since standard evaluation modules are **not compatible**, use the **"Execute Python Script" module** to run evaluation scripts for the custom model

[![Screenshot-2024-05-16-at-1-07-53-AM.png](https://i.postimg.cc/nhyTRFf9/Screenshot-2024-05-16-at-1-07-53-AM.png)](https://postimg.cc/4Y1znTdX)
[![Screenshot-2024-05-16-at-1-08-23-AM.png](https://i.postimg.cc/Kj8dGGzv/Screenshot-2024-05-16-at-1-08-23-AM.png)](https://postimg.cc/7GjBXy0p)

> ### <a href="/posts_blogs/blogs_dp100/Lab3CreateBasicPipeline" style="color:skyblue;" rel="noopener">Create a basic pipeline in Azure ML Studio - Lab 3</a>

> ### Filter Based Feature Selection Module

- **Configuration Options:**
  - Operate on Feature Columns only - True/False
  - Number of desires features (Specify the number of features to output in the results): 1
  - Feature scoring method - **PearsonCorrelation/ChiSquared**
  - Target Column: Specify target column

### Pearson Correlation vs Chi-Squared Statistics

| Aspect                      | Pearson Correlation                                  | Chi-Squared Statistics                                    |
|-----------------------------|-----------------------------------------------------|----------------------------------------------------------|
| **Purpose**                 | Measures the strength and direction of a linear **relationship** between two quantitative variables | It is a comparative test that reveals **how close expected values are to actual results**. |
| **Correlation Coefficient (R)** | Ranges from -1 to +1: <br> - **0**: No correlation <br> - **+1**: Perfect positive correlation <br> - **-1**: Perfect negative correlation | **No correlation coefficient**; lower values indicate a better fit to expected values |
| **Type of Variables**       | **Quantitative**                                        | **Categorical**                                              |
| **Interpretation**          | - **Positive Correlation**: e.g., More rain increases humidity <br> - **Negative Correlation**: e.g., Higher altitude decreases temperature | **Indicates** whether a **relationship exists** but does **not specify the type** (positive or negative) |
| **Value Indication**        | Indicates the strength of the relationship          | Smaller values indicate a better fit and existence of a relationship |
| **Steps**                   | 1. Determine linearity <br> 2. Clean data <br> 3. Generate the coefficient <br> 4. Evaluate the results |  |

> ### Permutation Feature Importance Module

It refers to **randomly shuffling data while keeping everything else constant** and then seeing if we have a change in whatever feature column we were looking at. From that we can generate new prediction based on results. Compute feature importance score by calculating decrease in quality

- **Configuration Options:**
  - **Random seed** (Random number generator seed value - It is going to **randomize the data**) = 1023 (Say)
  - **Metric for measuring performance:**
    - **Classification metrics** - **Accuracy** (how accurate model is), **Precision** (how good model is), **Recall** (how many times model was able to detect a specific category)
    - **Regression metrics** - **Mean Absolute Error**, **Root Mean Squared Error**, **Relative Absolute Error**, **Relative Squared Error**, **Coefficient of Determination** (R squared)

> ### Applying Automated ML to Explore Models

Automated ML democratizes machine learning with a **no-code approach**, making it easy to explore optimal machine learning models. Automated ML handles **preprocessing, featurization, transformation, scaling,** and **normalization**. At the end, it **scores** the model by selecting a metric and **deploying** the model. Automated ML is used for exploring optimal algorithms and parameters to **solve** a particular problem **without a lot of human trial-and-error**.

- **Types of algorithm** in automated ML: **Classification**, **Regression** and **Time Series Forecasting** 

**Examples of Automated ML**:
- Tabular Data
- Computer Vision
- Natural Language Processing

**Steps to Apply Automated ML**

- Step 1: **Create ML Client**

- Step 2: **Define MLTable**: (ML Table is already inputted)

- Step 3: **Define the AutoML Job**

```python
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import automl, Input

# Create an Input object for the training data
my_training_input = Input(
    type=AssetTypes.MLTABLE, 
    path="./data/training-mltable-folder"
)

# Configure the classification job
my_classification_job = automl.classification(
    compute="aemcmlcompute",
    experiment_name="this_experiment",
    training_data=my_training_input,
    target_column_name="try_me",
    primary_metric="accuracy",
    n_cross_validations=4,
    enable_model_explainability=True,
    tags={"my_tag": "My value"}
)

# Set optional limits
my_classification_job.set_limits(
    timeout_minutes=600,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True
)

# Set optional training properties
my_classification_job.set_training(
    blocked_training_algorithms=["logistic_regression"], 
    enable_onnx_compatible_models=True
)
```

- Step 4: **Submit (Run) the Job**

```python
# Submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    my_classification_job
)
returned_job.services ["Studio"].endpoint
```

> ### <a href="/posts_blogs/blogs_dp100/Lab4AzureAutomatedML" style="color:skyblue;" rel="noopener">(B) Build & Train Models using <u>Azure Automated Machine Learning</u> - Lab 4</a>*

> ### <a href="/posts_blogs/blogs_dp100/TrainModelUsingPythonSDK" style="color:skyblue;" rel="noopener">(C) Training Model by using Python SDK</a>*

[![Screenshot-2024-05-17-at-2-06-10-AM.png](https://i.postimg.cc/5NPfWhNd/Screenshot-2024-05-17-at-2-06-10-AM.png)](https://postimg.cc/kBt0xhbf)

> ### Tuning Hyperparameters in Azure Machine Learning*

**Hyperparameters**: **Top-level settings** you configure before running the ML algorithm, such as: train-test ratios, number of epochs, batch size

Types of Hyperparameters

- **Discrete Hyperparameters**: Specified as a **choice** among **discrete values** - Choose from a set of explicit values (e.g., 1, 2, 3). E.g. QUniform distribution

```python
{
    "batch_size": choice(1, 2, 3, 4)
    "number_of_hidden_layers": choice(range(1,5))
}
```

- **Continuous Hyperparameters**:  Specified from a **continuous (sliding) range of values** - Defined over a range, sampled from distributions:
  - Normal distribution
  - Uniform distribution
  - Lognormal and loguniform distributions

```python
{
    "learning_rate": normal(10,3),
    "keep_probability":uniform(0.05, 0.1)
}
```

- **Hyperparameter tuning:** Process of finding the **configuration of Hyperparameters** that results in the **best performance** 

- There are different **types of jobs** depending on how you want to execute a workload:
  - **Command:** Execute/**run a single script**.
  - **Sweep Job:** Perform **hyperparameter tuning** when executing a **single script**. Helps you **automate choosing** these **parameters**
  - **Pipeline:** **Run a pipeline** consisting of **multiple scripts** or components.

- **Note:** When you submit a **pipeline** you **created with designer** it will run as a **pipeline job**. When you submit an **Automated ML experiment**, it will also **run as a job**.

- **Sampling Methods**
[![Screenshot-2024-05-16-at-10-45-10-PM.png](https://i.postimg.cc/0yVpZMpP/Screenshot-2024-05-16-at-10-45-10-PM.png)](https://postimg.cc/RWn6Z0sD)

- **_<u>Define Primary Metric</u>_**

  - You can **define the objective** of your **sweep job** by specifying the primary metric and **goal** you want hyperparameter tuning to optimize. 
  - Each training job is evaluated for the primary metric.

    - **_primary_metric:_** The name of the primary metric and the name of the metric logged by the training script should be an exact match.
    - **_goal:_** It can be either **Maximize** or **Minimize**. This determines if the primary metric will be maximized or minimized during job evaluation.

```python
from azure.ai.ml.sweep import Uniform, Choice

command_job_for_sweep = command_job(
    learning_rate=Uniform ( min_value=0.05, max_value=0.1), 
                            batch_size = Choice (values = [16, 32, 64, 128]),
                            # Discrete hyperparameter used here
)

sweep_job = command_job_for_sweep.sweep(
    compute="cpu-cluster",
    sampling_algorithm = "bayesian", 
    primary_metric="accuracy",
    goal="Maximize",
)
```

> ### Early Termination Policies*

- **Increase efficiency** and **reduce costs** by stopping underperforming runs early.

- **_<u>Bandit Policy</u>_**: 
  - Terminates when **primary metric** is **not within** a specified **slack** factor/slack amount 
  - **Stops runs** that perform **worse than the best run by a specified margin**
  - **Slack amount** = 0.2 means that performance if less than 20 percent of best run, then run stops

```python
from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy
            (slack_amount = 0.2, evaluation_interval=1, delay_evaluation=5)
```

- **_<u>Truncation Selection Policy</u>_**: 
  - Terminates a percentage of **lowest-performing runs** at each evaluation interval
  - **Stops** the **lowest-performing runs** at each intervals

```python
from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelection Policy
            (truncation_percentage=25, evaluation_interval=1, delay_evaluation=2)
```

- **_<u>Median Stopping Policy</u>_**: 
  -  Terminates when **primary metric** value is **worse than median of averages**

```python
from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy
            (evaluation_interval=1, delay_evaluation=2)
```

- **_<u>No Termination Policy</u>_**
  - **Allows all** training runs execute to completion
  - Done by hyperparameter tuning service

> ### <a href="/posts_blogs/blogs_dp100/OptimizeHyperparameters" style="color:skyblue;" rel="noopener">Steps to Optimize Hyperparameters</a>

> ### Responsible AI Dashboard & Evaluate Automated ML run including Responsible AI*

- **Responsible AI Dashboard**

| Model debugging           | Business decision making   |
|---------------------------|----------------------------|
| Error analysis            | Causal analysis            |
| Data explorer             | Counterfactual what-if     |
| Model overview            |                            |
| Fairness assessment       |                            |
| Model interpretability    |                            |
| Counterfactual what-if    |                            |

[![Screenshot-2024-05-17-at-2-49-51-AM.png](https://i.postimg.cc/vZxqQKwS/Screenshot-2024-05-17-at-2-49-51-AM.png)](https://postimg.cc/jDKQcZ5z)



 









