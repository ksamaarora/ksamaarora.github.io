---
layout: post
title:  "Designing and Implementing Data Science Solution on Azure (DP-100)"
date:   2024-05-08 09:29:20 +0700
tags:
  - cloud
categories: jekyll update
usemathjax: true
---

> ### <a href="/posts_blogs/blogs_dp100/datasciencelifecycle" style="color:skyblue;" rel="noopener">Data Science Life Cycle & Responsible AI Guidelines</a>

> ### Azure Machine Learning Model 

[![Screenshot-2024-05-12-at-3-53-44-PM.png](https://i.postimg.cc/mkC3Mbcq/Screenshot-2024-05-12-at-3-53-44-PM.png)](https://postimg.cc/p5VnMMJY)

> ### Designing a Data Ingestion Solution*

- **Overview:** Extract **raw data** from source (CRM or IoT device) -> Copy and transform data with **Azure Synapse Analytics** -> **Store** prepared data in **Azure Blob Storage** -> **Train** model with **Azure ML**

| **Type of Data**       | **Description**                                                                                                                           | **Example**                                                                                                           |
|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Tabular or Structured Data**  | All data has the **same fields** or properties, defined in a schema. Represented in **tables** where **columns** are features and **rows** are data points. | Excel or CSV file |
| **Semi-structured Data** | **Not all data** has the **same fields** or properties. Represented by a collection of **key-value pairs**. Keys are features, and values are properties. | JSON object from IoT device: <br> { "deviceId": 29482, "location": "Office1", "time":"2021-07-14T12:47:39Z", "temperature": 23 } |
| **Unstructured Data**  | Files that **don't adhere to any rules** regarding **structure**. **Can't query** the data in the database. | Document, image, audio, or video file |

> **Store data for model training workloads:*** 

- When using Azure Machine Learning, Azure Databricks, or Azure Synapse Analytics for model training, three common options for storing data are:

  - **Azure Blob Storage:** Cheapest option for **storing unstructured data**. Ideal for storing files like images, text, CSV, and JSON. 

  - **Azure Data Lake Storage (Gen 2):** Best for **storing large volumes (limitless) of unstructured data** like CSV files and images. A data lake also implements a **hierarchical namespace**, which means it’s **easier** to give **someone access** to a **specific file or folder (privacy-sensitive)**. 

  - **Azure SQL Database:** **Stores unstructured data** that **doesn’t change over time**. Data is read as a table and schema is defined when a table in the database is created.

> **Create a data ingestion pipeline:***

- **Azure Synapse Analytics:** 
  - Used to create and schedule data ingestion pipelines through the **easy-to-use UI**, or by defining the pipeline in JSON format.
  - UI tool like **mapping data flow** or using a language like SQL, Python, or R.
  - Creates **automated pipelines** to **copy and move** data
  - Allows to choose between different compute - serverless SQL pools, dedicated SQL pools, or Spark pools

- **Azure Databricks:** 
  - Opt for this if you prefer a **code-first tool** and to use **SQL, python or R** to create pipelines. 
  - Azure Databricks **uses PySark**, which **distribute compute** to **transform large amounts of data** in **less time**.

- **Azure Machine Learning:** 
  - Provides **compute clusters** (automatically scale up or down when needed). 
  - Used to **manage all tasks with a single tool** or run pipelines with **on-demand compute cluster**. 
  - **Python** is preffered language 
  - Can create **pipeline using Designer**. 
  - **Intutive user interface**

- **Azure AI Services:**
  - Collection of **pre-built ML models**, this saves time and effort to train model
  - Models are offered as an **API** (Application Programming Interface)

> ### Creating an Azure ML Workspace

[![Screenshot-2024-05-15-at-3-43-17-AM.png](https://i.postimg.cc/xdx59S0d/Screenshot-2024-05-15-at-3-43-17-AM.png)](https://postimg.cc/d7Cr9pZM)

[![Screenshot-2024-05-15-at-4-08-43-AM.png](https://i.postimg.cc/4NCC9yMV/Screenshot-2024-05-15-at-4-08-43-AM.png)](https://postimg.cc/vDvSRYGZ)

- The following are **provisioned by default** when you **deploy** an Azure Machine Learning **workspace**. 
  - **Application Insights** is used to monitor predictive services in the workspace.
  - **Azure key vault** is used to securely manage & store secrets such as authentication keys and credentials used by the workspace. 
  - **Azure storage account** is used to store machine learning notebooks and logs related to machine learning jobs and other activities.
  - **Azure Container Registery** is created when needed to store images for Azure ML environments

> #### **_Using Python SDK to interact with Azure ML workspace_***
  - **Install Python SDK:**
  ```python
  pip install azure-ai-ml
  ```
  - **Connect to Workspace**: We **authenticate** the **MLClient** is used to connect to the workspace. To authenticate, we need **3 parameters** - subscription_id, resource_group and workspace_name
```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
```
  - **Create New Machine Learning Workspace**
```python
from azureml.core import Workspace
my_ws = Workspace(
    name="another-workspace",
    location="westus",
    display_name="another-workspace for you to view", 
    description="This is just an example"
)
ml_client.workspaces.begin_create(my_ws)
```
  - **Sample code to configure and run a job**
```python
from azure.ai.ml import command
# configure job
job = command(
    code="./src",
    command="python train.py",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    experiment_name="train-model"
)
# connect to workspace and submit job
returned_job = ml_client.create_or_update(job)
# Print the URL to monitor the job
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

> **_Using Azure CLI to interact with Azure ML workspace_**
- **Remove any ML CLI extensions** (Both version 1 and 2) to avoid conflicts with previous versions 
```bash
  az extension remove -n azure-cli-ml 
  az extension remove -n ml
```
- **Install** Azure ML (v2) extension 
```bash
az extension add -n ml -y
```
- **Create resource group** and choose a location close to you
```bash
az group create --name "rg-dp100-labs" --location "eastus"
```
- **Create a workspace**
```bash
az ml workspace create --name "mlw-dp100-labs" -g "rg-dp100-labs"
```
- Wait for workspace and associated resources to be created 

> #### Access in Azure ML Workspace*

- Access is granted in Azure using **role-based access control (RBAC)**, which can configure Access control tab of resource or resource group. You can manage permissions to restrict what actions certain users or teams can perform **3 built-in roles** you can use across resources and resource groups to assign permissions to other users:
  - **Owner:** Gets full access to all resources, and can grant access to others using access control.
  - **Contributor:** Gets full access to all resources, but can't grant access to others.
  - **Reader:** Can only view the resource, but isn't allowed to make any changes.
- Additionally, **Azure ML** has specific **built-in roles** you can use:
  - **AzureML Data Scientist:** Can perform all actions within the workspace, except for creating or deleting compute resources, or editing the workspace settings.
  - **AzureML Compute Operator:** Is allowed to create, change, and manage access the compute resources within a workspace.

> ### Resources and Assets*

<!-- [![Screenshot-2024-05-12-at-5-50-16-PM.png](https://i.postimg.cc/N0DZPCqJ/Screenshot-2024-05-12-at-5-50-16-PM.png)](https://postimg.cc/7CCB5VL0) -->

- **Resources** provide the **infrastructure and services** to build solutions, and **assets** are the **products or content created or configured** by data scientists and data engineers.
Examples of resources include compute and datastore, and examples of assets are machine learning models and data assets.

- A **compute target** is a designated **compute resource** or **environment** where you run your training script or host your service deployment. You **can use** a **different compute target** for each phase of your project. For example, you might use a compute instance to train your model and Kubernetes for a deployed model. Compute targets can be defined for **training as well as production phases** of your Azure Machine Learning projects.

- **_<u>Azure ML Resources:</u>_**

  > **Workspace** (explained above)

  > **Compute Resources**: 5 types - Compute Instance, Compute Clusters, Kubernetes Clusters, Attached Compute, Serverless Compute

  > **Datastores**

- **Create a new Datastore**: 

```python
from azureml.core import Datastore, AzureBlobDatastore 
a_blob_store= AzureBlobDatastore(
    name="my_blob_example",
    description="Datastore pointing to a blob container.",
    account_name="myexistingblobstore", 
    container_name="stuffandsuch-container",
    credentials={
        "account_key": "XXXXYYYXXXzzzYYYZZZxxx"
    },
)
ml_client.create_or_update(a_blob_store)
```

| Datastore                | Description                                                                               | Storage Type     | Main Usage                        |
|--------------------------|-------------------------------------------------------------------------------------------|------------------|-----------------------------------|
| **workspaceartifactstore** | Stores compute and experiment logs                                                       | Azure Blob       | Logs for jobs                     |
| **workspaceworkingdirectory** | Stores files uploaded via the Notebooks section                                         | Azure File Share | Notebook files                    |
| **workspaceblobstore**   | Default datastore for storing uploaded data                                                | Azure Blob       | Data assets                       |
| **workspacefilestore**   | General file storage                                                                       | Azure File Share | General file storage              |

  - **_Azure ML Assets:_**
    - Models
    - Environments
    - Data
    - Compoenents

### (I) _<u>DESIGN AND PREPARE A MACHINE LEARNING SOLUTION</u>_

> ### Compute*

| **Compute Type** | **Description** |
|---|---|
| **Compute Instances** | Similar to a virtual machine in the cloud, managed by the workspace. Ideal to use as a development environment to run (Jupyter) notebooks. |
| **Compute Clusters** | On-demand clusters of CPU or GPU compute nodes in the cloud, managed by the workspace. Ideal to use for production workloads as they automatically scale to your needs (increase/decrease number of nodes). Used for large runs. |
| | - **Dedicated Cluster:** These are just ready when you are - more expensive |
| | - **Low-Priority Cluster:** These systems are going to be ready when you are, probably. Low priority clusters may be accessed by multiple users thus you may get access to it but just in a few minutes or so - less expensive |
| **Inference Cluster/Kubernetes Cluster** | Allows you to create or attach an Azure Kubernetes Service (AKS) cluster. Ideal to deploy trained machine learning models in production scenarios. |
| **Attached Compute** | Allows you to attach other Azure compute resources to the workspace, like Azure Databricks, HDInsight cluster or Synapse Spark pools. Used for Specialized needs. |
| **Serverless Compute** | A fully managed, on-demand compute you can use for training jobs. |

- **_CPU or GPU_** *
  - **CPU** (Central Processing Unit) - sufficient and cheaper to use for smaller tabular datasets -
  - **GPU** (Graphical Processing Unit) - powerful and effective for unstructured data - for larger amount of tabular data - libraries such has RAPIDs (developed by NVIDIA) allow data prep and training with large datasets
  
- **_General purpose or memory optimized_** *
  - When creating compute resources, there are 2 VM sizes you choose from  
    - **General purpose:** Have a balanced CPU-to-memory ratio. Ideal for testing and development with smaller datasets.
    - **Memory optimized:** Have a high memory-to-CPU ratio. Great for in-memory analytics, which is ideal when you have larger datasets or when you're working in notebooks.

- **_Spark Compute / Clusters_** *
  - Spark cluster consists of **driver node** and **worker nodes**. Code will initially communicate with driver node. The work is then distributed across the worker nodes. This reduces processing time. Finally work is summarized and the driver node communicates the result back to you. 
  - Code needs to be written in **Spar-friendly language** like **Scala, SQK, Rspark, or PySpark** in order to distribute the workload. 

> ### Apache Spark Tools as Compute Targets

**Azure Synapse:** It is an enterprise analytics service platform that enables data warehousing, analytics, processing and integration and pipeline framed with a massively parallel processing architecture. Synapse supports bot SQL and Spark technologies. 

**Azure Synapse Spark Pools:**

- When setting up a Synapse Spark pool as an attached compute target in Azure Machine Learning studio:
Select an existing Azure Synapse workspace and an existing Spark pool in that workspace -> tick the option to set up a **managed identity** -> Choose system-assigned or user-assigned -> To **reliably connect to your new compute resource** to run workloads, Go to synapse studio and **assign managed identity in Azure ML to role of Synapse Administrator**. 

- **Serverless Spark Pools** can be used as a form of compute to set up notebooks in Azure ML Studio

> ### Create Compute Targets for Experiments and Training:*

Azure ML compute instance or compute clusters can be created from:
  - **Azure ML Studio**
  - **Python SDK**
  - **Azure CLI**
  - **Azure Resource Manager Templates** (You can re-use compute from the ARM templates)

> **_Using Python SDK:_**
  - Create **Compute instance**
```python
# Compute Instances need to have a unique name across the region.
# Here we create a unique name with current datetime
from azure.ai.ml.entities import ComputeInstance, AmlCompute
ci_basic_name = "basic-ci"
ci_basic = ComputeInstance(name=ci_basic_name, size="STANDARD_DS3_v2")
ml_client.begin_create_or_update(ci_basic)
```
  - Create **Compute cluster:** Specify **size and min max instances** required. This means when the VM is **idle** it ill resize to **min instances** and whenever in use it will scale up to max instances. 
```python
from azure.ai.ml.entities import AmlCompute
cluster_basic = AmlCompute(
name="basic-example",
type="amlcompute",
size="STANDARD_DS3_v2",
location="westus",
min_instances=0,
max_instances=2,
idle_time_before_scale_down=120,
)
ml_client.begin_create_or_update(cluster_basic)
```

> **_Using Azure CLI:_***
- **_Create Compute instance using Azure CLI_** 
  - Settings:
    - **Compute name:** Name of compute instance. Has to be unique and fewer than 24 characters.
    - **Virtual machine size:** STANDARD_DS11_V2
    - **Compute type (instance or cluster):** ComputeInstance
    - **Azure Machine Learning workspace name:** mlw-dp100-labs
    - **Resource group:** rg-dp100-labs
```bash
az ml compute create --name "ci1572" --size STANDARD_DS11_V2 --type ComputeInstance -w mlw-dp100-labs -g rg-dp100-labs
```
- **_Create compute cluster with Azure CLI_**
  - Settings
    - **Compute name:** aml-cluster
    - **Virtual machine size:** STANDARD_DS11_V2
    - **Compute type:** AmlCompute (Creates a compute cluster)
    - **Maximum instances:** Maximum number of nodes
    - **Azure Machine Learning workspace name:** mlw-dp100-labs
    - **Resource group:** rg-dp100-labs
```bash
az ml compute create --name "aml-cluster" --size STANDARD_DS11_V2 --max-instances 2 --type AmlCompute -w mlw-dp100-labs -g rg-dp100-labs
```
- **_Create compute target_**
  ```bash
  az ml compute create --name aml-cluster --size STANDARD_DS3_v2 --min-instances 0 --max-instances 5 --type AmlCompute --resource-group my-resource-group --workspace-name my-workspace
  ```
- Can use **YAML files** to **define configuration** - becomes easier to organize and automate tasks
  - **NOTE:** Designed for **automating tasks**. By using YAML files to define how the **model must be trained**, the **ML tasks** will be **repeatable**, **consistent** and **reliable**.
```yml
  $schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json 
  name: aml-cluster
  type: amlcompute
  size: STANDARD_DS3_v2
  min_instances: 0
  max_instances: 5
```
- When u saved the YAML file as compute.yml, you can create compute target by following command
```bash
  az ml compute create --file compute.yml --resource-group my-resource-group --workspace-name my-workspace
```

> ### Configure Attached Compute Resources*

[![Screenshot-2024-05-15-at-3-42-05-PM.png](https://i.postimg.cc/x1vy5nFd/Screenshot-2024-05-15-at-3-42-05-PM.png)](https://postimg.cc/bZvtNcPX)

> ### Selecting development approaches to build or train models*

Three ways to interact with Azure Machine Learning
  - **Azure CLI:** Use this command-line approach for automation of infrastructure. 
  - **Python SDK:** Used to submit jobs and manage models from a Jupyter notebook, ideal for data scientists - **code-first solutions**
  - **Azure ML Studio:** Use the user-friendly UI to explore workspace and other capabilities - **low code** training and development, data management and monitoring can be applied **(automated ML & visual designer)**

[![Screenshot-2024-05-13-at-3-31-18-AM.png](https://i.postimg.cc/2SHqc7ZD/Screenshot-2024-05-13-at-3-31-18-AM.png)](https://postimg.cc/hXdPhxsy)

- **Git:** Azure ML Workspaces work indirectly with Git for **source control**, and you can use a local Git repository by cloning it
  - ```python
  git clone url_of_repository
  ```

[![Screenshot-2024-05-15-at-4-11-56-AM.png](https://i.postimg.cc/QxNQSkR3/Screenshot-2024-05-15-at-4-11-56-AM.png)](https://postimg.cc/14TV36mC)

> #### Azure Machine Learning Environments*

**Environment:** Can be another form of **source control** and multiple custom environments can be created or curated environments can be used. Contains python packages, environment variables, software settings, runtimes. Environment ensures version control, reproducible and auditable. 

- **Environment** are divided into **3 major categories:**
  - **Curated Environment:** These are available in the workspace by default and are intended to be used as it is. These environements are **created and updated by Microsoft**.
  - **User-Managed Environment:** **You are responsible** to set up the environement as install packages required for training the script. You are responsible for everything. 
  - **System Managed:** In this, **Conda package manager** will **manage** the python environment for you.

- **Create List Environments**
```python
envs = ml_client.environments.list()
for env in envs:
    print(env.name)
```

- **Create a Custom Environment** 
```python
from azureml.core import Environment
my_env = Environment(
    image="pytorch/pytorch: latest", # base image to use
    name="docker-image-example", # name of the model
    description="Environment created from a Docker image.",
)
ml_client.environments.create_or_update(my_env) # use the MLClient to connect to workspace and create/register the environment
```

> ### Attached Compute - HD Insights and Apache Spark

```python
from azureml.core import RemoteCompute, ComputeTarget
# Ubuntu VMs only
# VM must have public IP addy
my_resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
my_compute_target_name = "attached_existingVM"
attached_target_config = RemoteCompute.attach
#
attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',
                                                    ssh_port=22,
                                                    username= '<username>'
                                                    password="<password>")
# Attach the compute
compute = ComputeTarget.attach(my_ws, my_compute_target_name, attached_target_config)
compute.wait_for_completion (show_output=True)
```

**Note:** When considering attaching an existing virtual machine to your Azure ML workspace as a compute target, it's crucial that the **external Vms** must be **Ubuntu only** and must have an **public IP address only**.  
However, the primary reason for **choosing an existing VM** over a new compute instance is to **utilize unused capacity effectively**.

```python
from azureml.core import ScriptRunConfig
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
my_env = Environment(name="mycoenv")

# Dependencies
my_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])

# Base Image. Leave out to use the default image: "azureml.core.runconfig.DEFAULT_CPU_IMAGE"

# Configure with the existing VM as the compute target, using the environment we just defined 
src = ScriptRunConfig(source_directory=".", script="train.py", compute_target=compute, environment=my_env)
```

> ### <a href="/posts_blogs/blogs_dp100/Lab1CreateMLWorkspace" style="color:skyblue;" rel="noopener">Create Azure Machine Learning Workspace - Lab 1</a>

### (II) _<u>EXPLORE DATA AND TRAIN MODEL</u>_

> ### Managing and Exploring Data Assets 

- **Datastores:** Datastores are **reference** to existing **Azure Storage resource** and are used to **read data directly** from that source
[![Screenshot-2024-05-15-at-4-15-45-AM.png](https://i.postimg.cc/02rLFmwT/Screenshot-2024-05-15-at-4-15-45-AM.png)](https://postimg.cc/w78fs1F2)
- **Datasets:** Datasets are also **references** but are **further abstracted** to point to **specific versioned** sets of data.
- **Data Asset Management:** It is the implementation and monitoring of datastores and datasets. It is version and tracking. It is registering and retrieveing those versions. It is how we are monitoring datasets and how we look at drift detection. We can access datasets that Azure provides and look at public datasets
- **Valid asset types** can be **folders, files** or **ml table**

> **Create and Manage Data Assets Using the SDK:***
<!-- [![Screenshot-2024-05-14-at-2-45-42-AM.png](https://i.postimg.cc/1Xkc4wC7/Screenshot-2024-05-14-at-2-45-42-AM.png)](https://postimg.cc/wRksGydD) -->
[![Screenshot-2024-05-22-at-3-23-33-PM.png](https://i.postimg.cc/wvTwRdbf/Screenshot-2024-05-22-at-3-23-33-PM.png)](https://postimg.cc/94Kdss2Z)

> ### Mounting and Downloading Files for Datasets 

|                     | Mount Files                                         | Download Files                                        |
|---------------------|-----------------------------------------------------|-------------------------------------------------------|
| **Description**         | Files do not reside in compute                     | Files downloaded to compute                           |
| **Processing**          | More streaming - means more processing/moving of data| Less streaming - less processing as data has been downloaded |
| **Usage**               | Good if you don’t use all files from dataset       | Good if you use all files from dataset               |
| **Available for**       | Datasets created from ADLS, SQL, Database, PostgreSQL| Datasets created from ADLS, SQL, Database, PostgreSQL|

<!-- > **Managing Data Assets Using the SDK:**
[![Screenshot-2024-05-14-at-3-02-55-AM.png](https://i.postimg.cc/rpp9zVy4/Screenshot-2024-05-14-at-3-02-55-AM.png)](https://postimg.cc/18LFjZqR) -->

**In Azure ML Studio:**

- Create Datastore
```python
from azureml.core import Workspace, Datastore
ws = Workspace.get(name='DP100Testing', subscription_id='5fb9753f-1afe-4b8e-a65c-e402ecd9f0a5', resource_group='AZ800')
blob_ds = Datastore.register_azure_blob_container(
    workspace = WS, datastore_name='example',
    container_name='azureml',
    account_name='dp100testing1952224325',
    account_key='lxt3bpnDPOj zcN1081Pg38wMXI0QzRYlbbZnvghwGd82A518uj 5NTZGWZr7F/120jZvVuVGQ4XSk+ASt91vKog=='
)
```

- Print Datastore 
```python
for ds_name in ws.datastores:
    print(ds_name)
```

- Create dataset
```python
VERSION = "1"
my_data = Data(
    path = https://dp100testing1952224325.blob.core.windows.net/azureml/AdventureWorks Sales-2.xlsx,
    type = AssetTypes.URI_FILE,
    description = "Sales",
    name = "AW-Sales",
    version = VERSION
)
ml_client.data.create_or_update(my_data)
```

> ### Preprocessing of Data 

**Steps** for Preprocessing of Data:
- **Data quality assessment**
- **Data Cleaning:** Look for Missing data, Noisy data
- **Data transformation:** Aggregation, Feature selection, Normalization (combining all the data so that everything is standardized)
- **Data reduction:** employ math to filter out unnecessary data

> ### Feature Selection and Feature Engineering

- **Feature Selection:** The process of **selecting specific variables (features)** that contribute the most to the prediction variable in our algorithms.

- **Feature Engineering:** The process of **selecting and expressing data** in a way that improves the performance of machine learning models. 
    - **Wrapper method:** Wrapper methods **evaluate subsets of features** by training a model on them and assessing the model's performance. All **combinations** are evaluated and the **best one** is chosen. Prone to overfitting
    - **Filter method:** Filter methods **apply statistical techniques** to evaluate the relevance of each feature independently from the machine learning model. They **assign a score to each feature** based on various statistical tests and **rank them accordingly**. Thus **Features** can then be **included or removed** from the dataset **based on their scores**. This method is **relationship based**. Faster than wrapper method. 

> ### Differential Privacy: Eg of Responsible AI*

**Differential Privacy** seeks to **protect individual data by adding statistical noise to the analysis process**. **Minimizing** risk of **personal identification** and **ensuring data privacy.** It ensures that the **output** of a data analysis algorithm does **not reveal sensitive information** about any individual.
- When applying differential privacy, **Epsilon (ε), Bounds and Sample size** needs to be **defined**
- **Privacy Loss Paramter/Epsilon (ε)** is a key parameter in differential privacy that **controls the balance between privacy and accuracy - Value ranges between 0 and 1**
  - **Low Epsilon (ε)**: **High privacy** - **Low accuracy** - More noise - High data obscurity - Data more difficult to interpret
  - **High Epsilon (ε)**: **Low Privacy** - **High accuracy** - Less noise - Less data obscurity - Data more accurate and easier to interpret 

```python
# Sample code snippet
privacy_usage = { 'epsilon': 0.10},
                data_lower = lower_range[1],
                data_upper = upper_range[10],
                data_rows = sample
```

> ### Accessing Data During Interactive Development

**Data Wrangling:** It is the process of **transforming data** to a format that’s best suited to the needs of ML model. 

- Step 1: Open a notebook with a running ML Azure Kernel

- Step 2: Create **ML Client** and a **datastore** (code mentioned before)

- Step 3: Build a **URI** (**Uniform Resource Identifier**: It specifies location of a resource) OR **Directly grab** it in the **Studio UI**

```python
# Azure Machine Learning workspace details:
subscription = '<subscription_id>'
resource_group = '<resource_group>'
workspace = '<workspace>'
datastore_name = '<datastore>'
path_on_datastore = '<path>'

# Long-form Datastore URI format:
uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}'
```

OR go to Data -> specific Datastore -> select csv file -> copy URI (Datastore URI or Storage URI)
  - **Note:** The **Datastore URI** is only **applicable to Azure ML** and the **Storage URI** is a more generic storage endpoint, which is used only **outside Azure ML**.

- Step 4: Load a **Pandas Dataframe**

```python
# Import pandas library
import pandas as pd

# Populate dataframe "my_dataframe" using the pandas read CSV method by passing in the URI acquired
my_dataframe = pd.read_csv("URI")

# Then run dataframe head passing in a value for the number of rows you want to return
my_dataframe.head(1000)
```

- Step 5: **Wrangle** - Replace **Missing** Strings

```python
# Fill missing values in the "Claim Network Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Claim Network Status": "Unkown"}, inplace=True)

# Fill missing values in the "Payment Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Payment Status": "Unkown"}, inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 6: **Wrangle** - **Delete Rows** With any **Empty** Columns

```python
# Drop rows with any missing values and update the dataframe in place
my_dataframe.dropna(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

> ### Wrangling Data with Apache Spark

- Step 1: Create a **compute** to power the notebook - Compute instance / Synapse Spark Pool / Azure ML Serverless Spark

- Step 2: Open a **notebook** and we use **Azure ML Serverless Spark** as compute for ease of use

- Step 3: Build a **URI** or grab it from the Studio UI

- Step 4: Load a **PySpark Pandas Dataframe**

```python
# Import pyspark pandas library
import pyspark.pandas as pd
my_dataframe = pd.read_csv("URI")
my_dataframe.head(1000)
```

- Step 5: Wrangle - Replace **Missing** Strings (code given above)

- Step 6: Wrangle - **Delete rows** with any **empty** columns (code given above)

- Step 7: **Wrangle** - Remove **Duplicate** Rows 

```python
# Drop duplicate rows and update the dataframe in place
my_dataframe.drop_duplicates(inplace=True)

# Sort the dataframe by its index and update the dataframe in place
my_dataframe.sort_index(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 8: **Save** the Transformed Data

```python
# Save the dataframe to a CSV file at the specified Azure Data Lake Storage path
my_dataframe.to_csv(
    "abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled"
    )
```

> ### <a href="/posts_blogs/blogs_dp100/Lab2WrangleData" style="color:skyblue;" rel="noopener">Wrangle Data with Python in Azure ML - Lab 2</a>

### (III) _<u>THREE WAYS TO BUILD AND TRAIN ML MODEL</u>_

> ### (A) Build & Train Models using <u>Azure ML Designer</u> 

- Step 1: Open **Azure ML Studio** -> Select **Designer**
- Step 2: Create new **pipeline (Classic Pre-built/Custom)**. Say we select a classic pre-built model for example - Regression Automobile Price prediction (Basic)
- Step 3: Now **run** this model by setting up a **pipeline job and a compute**
- Step 4: **Evaluate** the model based on metrics: **coefficient of determination, mean absolute error, relative absolute error, relative squared error, root mean squared error**

> **Dataset -> Select Columns in Dataset -> Clean Missing Data -> Split Data & (Linear Regression) -> Train Model -> Score Model -> Evaluate Model**

[![Screenshot-2024-05-16-at-1-06-58-AM.png](https://i.postimg.cc/26zyrWfp/Screenshot-2024-05-16-at-1-06-58-AM.png)](https://postimg.cc/B8V4CX6N)

> ### Custom Code Components

Custom code modules are created using Python. Supported libraries include NumPy, SciPy, scikit-learn, Theano, TensorFlow, Keras, PyTorch, pandas, and matplotlib

- **Create Custom Model:** Develop your custom model using **"Create Python Model"** module
- **Link** the custom model to the **"Train Model"** module to train it with your dataset
- Since standard evaluation modules are **not compatible**, use the **"Execute Python Script" module** to run evaluation scripts for the custom model

[![Screenshot-2024-05-16-at-1-07-53-AM.png](https://i.postimg.cc/nhyTRFf9/Screenshot-2024-05-16-at-1-07-53-AM.png)](https://postimg.cc/4Y1znTdX)
[![Screenshot-2024-05-16-at-1-08-23-AM.png](https://i.postimg.cc/Kj8dGGzv/Screenshot-2024-05-16-at-1-08-23-AM.png)](https://postimg.cc/7GjBXy0p)

> ### <a href="/posts_blogs/blogs_dp100/Lab3CreateBasicPipeline" style="color:skyblue;" rel="noopener">Create a basic pipeline in Azure ML Studio - Lab 3</a>

> ### Filter Based Feature Selection Module

- **Configuration Options:**
  - Operate on Feature Columns only - True/False
  - Number of desires features (Specify the number of features to output in the results): 1
  - Feature scoring method - **PearsonCorrelation/ChiSquared**
  - Target Column: Specify target column

### Pearson Correlation vs Chi-Squared Statistics

| Aspect                      | Pearson Correlation                                  | Chi-Squared Statistics                                    |
|-----------------------------|-----------------------------------------------------|----------------------------------------------------------|
| **Purpose**                 | Measures the strength and direction of a linear **relationship** between two quantitative variables | It is a comparative test that reveals **how close expected values are to actual results**. |
| **Correlation Coefficient (R)** | Ranges from -1 to +1: <br> - **0**: No correlation <br> - **+1**: Perfect positive correlation <br> - **-1**: Perfect negative correlation | **No correlation coefficient**; lower values indicate a better fit to expected values |
| **Type of Variables**       | **Quantitative**                                        | **Categorical**                                              |
| **Interpretation**          | - **Positive Correlation**: e.g., More rain increases humidity <br> - **Negative Correlation**: e.g., Higher altitude decreases temperature | **Indicates** whether a **relationship exists** but does **not specify the type** (positive or negative) |
| **Value Indication**        | Indicates the strength of the relationship          | Smaller values indicate a better fit and existence of a relationship |
| **Steps**                   | 1. Determine linearity <br> 2. Clean data <br> 3. Generate the coefficient <br> 4. Evaluate the results |  |

> ### Permutation Feature Importance Module

It refers to **randomly shuffling data while keeping everything else constant** and then seeing if we have a change in whatever feature column we were looking at. From that we can generate new prediction based on results. Compute feature importance score by calculating decrease in quality

- **Configuration Options:**
  - **Random seed** (Random number generator seed value - It is going to **randomize the data**) = 1023 (Say)
  - **Metric for measuring performance:**
    - **Classification metrics** - **Accuracy** (how accurate model is), **Precision** (how good model is), **Recall** (how many times model was able to detect a specific category)
    - **Regression metrics** - **Mean Absolute Error**, **Root Mean Squared Error**, **Relative Absolute Error**, **Relative Squared Error**, **Coefficient of Determination** (R squared)

> ### Applying Automated ML to Explore Models

Automated ML democratizes machine learning with a **no-code approach**, making it easy to explore optimal machine learning models. Automated ML handles **preprocessing, featurization, transformation, scaling,** and **normalization**. At the end, it **scores** the model by selecting a metric and **deploying** the model. Automated ML is used for exploring optimal algorithms and parameters to **solve** a particular problem **without a lot of human trial-and-error**.

- **Types of algorithm** in automated ML: **Classification**, **Regression** and **Time Series Forecasting** 

**Examples of Automated ML**:
- Tabular Data
- Computer Vision
- Natural Language Processing

**Steps to Apply Automated ML**

- Step 1: **Create ML Client**

- Step 2: **Define MLTable**: (ML Table is already inputted)

- Step 3: **Define the AutoML Job**

```python
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import automl, Input

# Create an Input object for the training data
my_training_input = Input(
    type=AssetTypes.MLTABLE, 
    path="./data/training-mltable-folder"
)

# Configure the classification job
my_classification_job = automl.classification(
    compute="aemcmlcompute",
    experiment_name="this_experiment",
    training_data=my_training_input,
    target_column_name="try_me",
    primary_metric="accuracy",
    n_cross_validations=4,
    enable_model_explainability=True,
    tags={"my_tag": "My value"}
)

# Set optional limits
my_classification_job.set_limits(
    timeout_minutes=600,
    trial_timeout_minutes=20,
    max_trials=5,
    enable_early_termination=True
)

# Set optional training properties
my_classification_job.set_training(
    blocked_training_algorithms=["logistic_regression"], 
    enable_onnx_compatible_models=True
)
```

- Step 4: **Submit (Run) the Job**

```python
# Submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    my_classification_job
)
returned_job.services ["Studio"].endpoint
```

> ### <a href="/posts_blogs/blogs_dp100/Lab4AzureAutomatedML" style="color:skyblue;" rel="noopener">(B) Build & Train Models using <u>Azure Automated Machine Learning</u> - Lab 4</a>*

> ### <a href="/posts_blogs/blogs_dp100/TrainModelUsingPythonSDK" style="color:skyblue;" rel="noopener">(C) Training Model by using Python SDK</a>*

[![Screenshot-2024-05-17-at-2-06-10-AM.png](https://i.postimg.cc/5NPfWhNd/Screenshot-2024-05-17-at-2-06-10-AM.png)](https://postimg.cc/kBt0xhbf)

> ### Tuning Hyperparameters in Azure Machine Learning*

**Hyperparameters**: **Top-level settings** you configure before running the ML algorithm, such as: train-test ratios, number of epochs, batch size

Types of Hyperparameters

- **Discrete Hyperparameters**: Specified as a **choice** among **discrete values** - Choose from a set of explicit values (e.g., 1, 2, 3). E.g. QUniform distribution

```python
{
    "batch_size": choice(1, 2, 3, 4)
    "number_of_hidden_layers": choice(range(1,5))
}
```

- **Continuous Hyperparameters**:  Specified from a **continuous (sliding) range of values** - Defined over a range, sampled from distributions:
  - Normal distribution
  - Uniform distribution
  - Lognormal and loguniform distributions

```python
{
    "learning_rate": normal(10,3),
    "keep_probability":uniform(0.05, 0.1)
}
```

- **Hyperparameter tuning:** Process of finding the **configuration of Hyperparameters** that results in the **best performance** 

- There are different **types of jobs** depending on how you want to execute a workload:
  - **Command:** Execute/**run a single script**.
  - **Sweep Job:** Perform **hyperparameter tuning** when executing a **single script**. Helps you **automate choosing** these **parameters**
  - **Pipeline:** **Run a pipeline** consisting of **multiple scripts** or components.

- **Note:** When you submit a **pipeline** you **created with designer** it will run as a **pipeline job**. When you submit an **Automated ML experiment**, it will also **run as a job**.

- **Sampling Methods**
[![Screenshot-2024-05-16-at-10-45-10-PM.png](https://i.postimg.cc/0yVpZMpP/Screenshot-2024-05-16-at-10-45-10-PM.png)](https://postimg.cc/RWn6Z0sD)

- **_<u>Define Primary Metric</u>_**

  - You can **define the objective** of your **sweep job** by specifying the primary metric and **goal** you want hyperparameter tuning to optimize. 
  - Each training job is evaluated for the primary metric.

    - **_primary_metric:_** The name of the primary metric and the name of the metric logged by the training script should be an exact match.
    - **_goal:_** It can be either **Maximize** or **Minimize**. This determines if the primary metric will be maximized or minimized during job evaluation.

```python
from azure.ai.ml.sweep import Uniform, Choice

command_job_for_sweep = command_job(
    learning_rate=Uniform ( min_value=0.05, max_value=0.1), 
                            batch_size = Choice (values = [16, 32, 64, 128]),
                            # Discrete hyperparameter used here
)

sweep_job = command_job_for_sweep.sweep(
    compute="cpu-cluster",
    sampling_algorithm = "bayesian", 
    primary_metric="accuracy",
    goal="Maximize",
)
```

> ### Early Termination Policies*

- **Increase efficiency** and **reduce costs** by stopping underperforming runs early.

- **_<u>Bandit Policy</u>_**: 
  - Terminates when **primary metric** is **not within** a specified **slack** factor/slack amount 
  - **Stops runs** that perform **worse than the best run by a specified margin**
  - **Slack amount** = 0.2 means that performance if less than 20 percent of best run, then run stops

```python
from azureml.train.hyperdrive import BanditPolicy

early_termination_policy = BanditPolicy
            (slack_amount = 0.2, evaluation_interval=1, delay_evaluation=5)
```

- **_<u>Truncation Selection Policy</u>_**: 
  - Terminates a percentage of **lowest-performing runs** at each evaluation interval
  - **Stops** the **lowest-performing runs** at each intervals

```python
from azureml.train.hyperdrive import TruncationSelectionPolicy

early_termination_policy = TruncationSelection Policy
            (truncation_percentage=25, evaluation_interval=1, delay_evaluation=2)
```

- **_<u>Median Stopping Policy</u>_**: 
  -  Terminates when **primary metric** value is **worse than median of averages**

```python
from azureml.train.hyperdrive import MedianStoppingPolicy

early_termination_policy = MedianStoppingPolicy
            (evaluation_interval=1, delay_evaluation=2)
```

- **_<u>No Termination Policy</u>_**
  - **Allows all** training runs execute to completion
  - Done by hyperparameter tuning service

> ### <a href="/posts_blogs/blogs_dp100/OptimizeHyperparameters" style="color:skyblue;" rel="noopener">Steps to Optimize Hyperparameters</a>

> ### Responsible AI Dashboard & Evaluate Automated ML run including Responsible AI*

- **Responsible AI Dashboard**

| Model debugging           | Business decision making   |
|---------------------------|----------------------------|
| Error analysis            | Causal analysis            |
| Data explorer             | Counterfactual what-if     |
| Model overview            |                            |
| Fairness assessment       |                            |
| Model interpretability    |                            |
| Counterfactual what-if    |                            |

[![Screenshot-2024-05-17-at-2-49-51-AM.png](https://i.postimg.cc/vZxqQKwS/Screenshot-2024-05-17-at-2-49-51-AM.png)](https://postimg.cc/jDKQcZ5z)



 









