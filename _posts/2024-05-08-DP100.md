---
layout: post
title:  "Designing and Implementing Data Science Solution on Azure (DP-100)"
date:   2024-05-31 09:29:20 +0700
tags:
  - cloud
categories: jekyll update
usemathjax: true
---

- <a href="/posts_blogs/blogs_dp100/datasciencelifecycle" style="color:skyblue;" rel="noopener">Data Science Life Cycle & Responsible AI Guidelines</a> - Overview

> ### Designing a Data Ingestion Solution*

**Overview:** Extract **raw data** from source (CRM or IoT device) -> Copy and transform data with **Azure Synapse Analytics** -> **Store** prepared data in **Azure Blob Storage** -> **Train** model with **Azure ML**

| **Type of Data**       | **Description**                                                                                                                           | **Example**                                                                                                           |
|------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Tabular or Structured Data**  | All data has the **same fields** or properties, defined in a schema. Represented in **tables** where **columns** are features and **rows** are data points. | **Excel** or **CSV** file |
| **Semi-structured Data** | **Not all data** has the **same fields** or properties. Represented by a collection of **key-value pairs**. Keys are features, and values are properties. | **JSON object from IoT** device: <br> { "deviceId": 29482, "location": "Office1", "time":"2021-07-14T12:47:39Z", "temperature": 23 } |
| **Unstructured Data**  | Files that **don't adhere to any rules** regarding **structure**. **Can't query** the data in the database. | **Document, image, audio, or video** file |

> **Store data for model training workloads:*** 

- When using Azure Machine Learning, Azure Databricks, or Azure Synapse Analytics for model training, three common options for storing data are:

  - **Azure Blob Storage:** Cheapest option for **storing unstructured data**. Ideal for storing files like images, text, CSV, and JSON 

  - **Azure Data Lake Storage (Gen 2):** Best for **storing large volumes (limitless) of unstructured data** like CSV files and images. A data lake also implements a **hierarchical namespace**, which means it’s **easier** to give **someone access** to a **specific file or folder (privacy-sensitive)**

  - **Azure SQL Database:** **Stores unstructured data** that **doesn’t change over time**. Data is read as a table and schema is defined when a table in the database is created.

> **Create a data ingestion pipeline:***

- **Azure Synapse Analytics:** 
  - Used to create and schedule data ingestion pipelines through the **easy-to-use UI**, or by defining the pipeline in JSON format.
  - UI tool like **mapping data flow** or using a language like SQL, Python, or R.
  - Creates **automated pipelines** to **copy and move** data
  - Allows to choose between different compute - serverless SQL pools, dedicated SQL pools, or Spark pools

- **Azure Databricks:** 
  - Opt for this if you prefer a **code-first tool** and to use **SQL, python or R** to create pipelines. 
  - Azure Databricks **uses PySark**, which **distribute compute** to **transform large amounts of data** in **less time**.

- **Azure Machine Learning:** 
  - Provides **compute clusters** (automatically scale up or down when needed). 
  - Used to **manage all tasks with a single tool** or run pipelines with **on-demand compute cluster**. 
  - **Python** is preffered language 
  - Can create **pipeline using Designer**. 
  - **Intutive user interface**

- **Azure AI Services:**
  - Collection of **pre-built ML models**, this saves time and effort to train model
  - Models are offered as an **API** (Application Programming Interface)

> ### Selecting development approaches to build or train models*

Three ways to interact with Azure Machine Learning
  - **Azure CLI:** Use this command-line approach for automation of infrastructure. 
  - **Python SDK:** Used to submit jobs and manage models from a Jupyter notebook, ideal for data scientists - **code-first solutions**
  - **Azure ML Studio:** Use the **user-friendly UI** to explore workspace and other capabilities - **low code** training and development, data management and monitoring can be applied **(automated ML & visual designer)**

[![Screenshot-2024-05-13-at-3-31-18-AM.png](https://i.postimg.cc/2SHqc7ZD/Screenshot-2024-05-13-at-3-31-18-AM.png)](https://postimg.cc/hXdPhxsy)

- **Git:** Azure ML Workspaces work indirectly with Git for **source control**, and you can **use** a **local** Git repository by **cloning** it (_git clone url_of_repository_)
[![Screenshot-2024-05-15-at-4-11-56-AM.png](https://i.postimg.cc/QxNQSkR3/Screenshot-2024-05-15-at-4-11-56-AM.png)](https://postimg.cc/14TV36mC)

> ### <a href="/posts_blogs/blogs_dp100/environments" style="color:skyblue;" rel="noopener">Azure Machine Learning Environments* </a> (Imp)

**Environment:** Can be another form of **source control** and multiple custom environments can be created or curated environments can be used. Contains python packages, environment variables, software settings, runtimes. Environment **ensures version control, reproducible and auditable**. 

- **Environment** are divided into **3 major categories:**
  - **Curated Environment:** These are **automatically created** when ML workspace is created **(pre-built environments)**. These environements are **created and updated by Microsoft**. Curated environments **use prefix AzureML-** - **faster development time**
  - **User-Managed Environment:** **You are responsible** to set up the environement as install packages required for training the script. You are responsible for everything. 
  - **System Managed:** In this, **Conda package manager** will **manage** the python environment for you.

> ### Azure ML Workspace

[![Screenshot-2024-05-15-at-3-43-17-AM.png](https://i.postimg.cc/xdx59S0d/Screenshot-2024-05-15-at-3-43-17-AM.png)](https://postimg.cc/d7Cr9pZM)

[![Screenshot-2024-05-15-at-4-08-43-AM.png](https://i.postimg.cc/4NCC9yMV/Screenshot-2024-05-15-at-4-08-43-AM.png)](https://postimg.cc/vDvSRYGZ)

- The following are **provisioned by default** when you **deploy** an Azure Machine Learning **workspace**. 
  - **Application Insights** is used to monitor predictive services in the workspace.
  - **Azure key vault** is used to securely manage & store secrets such as authentication keys and credentials used by the workspace. 
  - **Azure storage account** is used to store machine learning notebooks and logs related to machine learning jobs and other activities.
  - **Azure Container Registery** is created when needed to store images for Azure ML environments

- ### <a href="/posts_blogs/blogs_dp100/creatingAzureMLWorkspace" style="color:skyblue;" rel="noopener">Creating Azure ML Workspace using Python SDK and CLI</a> (Imp)

> ### Resources and Assets*

- **_<u>Azure ML Resources:</u>_** provide infrastructure and services to build solution

  > **Workspace** (explained above)

  > **Compute Resources**: 5 types - Compute Instance, Compute Clusters, Kubernetes Clusters, Attached Compute, Serverless Compute

  > **Datastores**

| Datastore                | Description                                                                               | Storage Type     | Main Usage                        |
|--------------------------|-------------------------------------------------------------------------------------------|------------------|-----------------------------------|
| **workspaceartifactstore** | Stores compute and experiment logs                                                       | Azure Blob       | Logs for jobs                     |
| **workspaceworkingdirectory** | Stores files uploaded via the Notebooks section                                         | Azure File Share | Notebook files                    |
| **workspaceblobstore**   | Default datastore for storing uploaded data                                                | Azure Blob       | Data assets                       |
| **workspacefilestore**   | General file storage                                                                       | Azure File Share | General file storage              |

  - **_Azure ML Assets:_** are product or contents created and configured by data scientists and engineers
    - Models
    - Environments
    - Data
    - Compoenents

### (I) _<u>DESIGN AND PREPARE A MACHINE LEARNING SOLUTION</u>_

> ### Compute*

- **Compute Target**: is a **compute resource/environment** used to train and host model. Can use different compute target for each phase (training/production) of project. 

| **Compute Type** | **Description** |
|---|---|
| **Compute Instances** | **Similar to a VM** - Primarily used to **run notebooks** - **Ideal for experimentation** - Easiest option to work with compute instance is through the **integrated notebooks experience** in the **Azure ML** studio. OR use **VS Code** for **easier source control** of your code. |
| **Compute Clusters** | **On-demand multi-node clusters** of **CPU or GPU** - **automatically** scale - used for **large volume** of **data** - allow for **parallel processing** to distribute workload - reduce time of run. - Ideal to use for **production workloads** |
| | - **Dedicated Cluster:** These are **just ready** when you are - more expensive |
| | - **Low-Priority Cluster:** These systems are **going to be ready** when you are, probably. Low priority clusters may be accessed by multiple users thus you may get access to it but just in a few minutes or so - less expensive |
| **Inference Cluster/Kubernetes Cluster** | Allows you to create or attach an **Azure Kubernetes Service (AKS) cluster**. Ideal to deploy trained machine learning models in production scenarios. |
| **Attached Compute** | **Allows** you **to attach other** Azure compute resources to the workspace, like Azure Databricks, HDInsight cluster or Synapse Spark pools. Used for Specialized needs. |
| **Serverless Compute** | A **fully managed, on-demand compute** you can use **for training jobs**. |

- **_Central Processing Unit (CPU) or Graphical Processing Unit (GPU)_** *
  - **CPU** - sufficient and **cheaper** to use for **smaller tabular datasets**
  - **GPU** - powerful and effective for **unstructured data** - for **larger** amount of tabular data - libraries such has **RAPIDs (developed by NVIDIA)** allow data prep and training with large datasets
  
- **_General purpose or memory optimized_** *
  - When creating compute resources, there are 2 VM sizes you choose from  
    - **General purpose:** Have a **balanced CPU-to-memory ratio**. Ideal for testing and development with **smaller datasets**.
    - **Memory optimized:** Have a **high memory-to-CPU ratio**. Great for **in-memory analytics**, which is ideal when you have **larger datasets** or when you're **working in notebooks**.

- **_Spark Compute / Clusters_** *
  - Spark cluster consists of **driver node** and **worker nodes**. Code will **initially** communicate with **driver** node. The work is then **distributed across the worker** nodes. This **reduces processing time**. Finally work is summarized and the driver node communicates the result back to you. 
  - Code needs to be written in **Spar-friendly language** like **Scala, SQK, Rspark, or PySpark** in order to distribute the workload. 

> ### Apache Spark Tools as Compute Targets

**Azure Synapse:** It is an **enterprise analytics service platform** that enables data warehousing, analytics, processing and integration and pipeline framed with a **massively parallel processing** architecture. **Synapse supports** bot **SQL and Spark** technologies. 

**Azure Synapse Spark Pools:**

- When setting up a Synapse Spark pool as an attached compute target in Azure Machine Learning studio:
Select an existing Azure Synapse workspace and an existing Spark pool in that workspace -> tick the option to set up a **managed identity** -> Choose system-assigned or user-assigned -> To **reliably connect to your new compute resource** to run workloads, Go to synapse studio and **assign managed identity in Azure ML to role of Synapse Administrator**. 

**Serverless Spark Pools** can be used as a **form of compute to set up notebooks** in Azure ML Studio

> ### Create Compute Targets for Experiments and Training:*

Azure ML compute instance or compute clusters can be created from:
  - **Azure ML Studio**
  - **Python SDK**
  - **Azure CLI**
  - **Azure Resource Manager Templates** (can re-use compute from the ARM templates)

### <a href="/posts_blogs/blogs_dp100/creatingComputeTarget" style="color:skyblue;" rel="noopener">Create Compute Target using Python SDK and CLI</a> (Imp)

> ### Configure Attached Compute Resources*

[![Screenshot-2024-05-15-at-3-42-05-PM.png](https://i.postimg.cc/x1vy5nFd/Screenshot-2024-05-15-at-3-42-05-PM.png)](https://postimg.cc/bZvtNcPX)

> ### Attached Compute - HD Insights and Apache Spark

```python
from azureml.core import RemoteCompute, ComputeTarget
# Ubuntu VMs only
# VM must have public IP addy
my_resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
my_compute_target_name = "attached_existingVM"
attached_target_config = RemoteCompute.attach
#
attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',
                                                    ssh_port=22,
                                                    username= '<username>'
                                                    password="<password>")
# Attach the compute
compute = ComputeTarget.attach(my_ws, my_compute_target_name, attached_target_config)
compute.wait_for_completion (show_output=True)
```

**Note:** When considering attaching an existing virtual machine to your Azure ML workspace as a compute target, it's crucial that the **external Vms** must be **Ubuntu only** and must have an **public IP address only**.  
However, the primary reason for **choosing an existing VM** over a new compute instance is to **utilize unused capacity effectively**.

<!-- 
```python
from azureml.core import ScriptRunConfig
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
my_env = Environment(name="mycoenv")

# Dependencies
my_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])

# Base Image. Leave out to use the default image: "azureml.core.runconfig.DEFAULT_CPU_IMAGE"

# Configure with the existing VM as the compute target, using the environment we just defined 
src = ScriptRunConfig(source_directory=".", script="train.py", compute_target=compute, environment=my_env)
``` -->

> ### <a href="/posts_blogs/blogs_dp100/Lab1CreateMLWorkspace" style="color:skyblue;" rel="noopener">Create Azure Machine Learning Workspace - Lab 1</a>

### (II) _<u>EXPLORE DATA AND TRAIN MODEL</u>_

> **Terminologies**:*

> **URI (Uniform Resource Identifier)**:* 

  - It specifies **location of data/resource**. 
  - To connect Azure ML to your data, a **protocol needs to be prefixed to the URI**. 
  - The **three common protocols** are:
    - **_http(s):_** Use for data stores **publicly or privately** in an **Azure Blob Storage** or **publicly available http(s)** location. (Protocol used when accessing data stored in publicly available http(s) location)
    - **_abfs(s):_** Use for data stores in an **Azure Data Lake Storage Gen 2**.
    - **_azureml:_** Use for **data stored** in a **datastore**.

> **Datastores**:* 

  - Datastores are **reference** to existing **Azure Storage resource** and are used to **read data directly** from that source.
  - **Benefits:**
    - Provides **easy-to-use URI** to your data storage
    - Facilitates **data discovery** within Azure ML
    - **Securely stores connection** information, **without exposing secrets and keys** to data scientists 
  - **Two methods for authentication:**
    - **Credential Based:** Use a service principal, **shared access signature (SAS) token** or **account key** to authenticate access to your storage account
    - **Identity-Based:** Use your **Microsoft Entra identity** or **managed identity**
  - **Types of datastores:**
  [![Screenshot-2024-05-15-at-4-15-45-AM.png](https://i.postimg.cc/02rLFmwT/Screenshot-2024-05-15-at-4-15-45-AM.png)](https://postimg.cc/w78fs1F2)

> **Data Assets:*** 

- They are **references to data in datastores**
- **Benefits**:
  - Can **share** and **reuse** data 
  - Can seamlessly access data during model training or **any supported compute** type
  - Can **version metadata** of the data asset
- **Three types of data assets:**
  - **URI file:** Points to specific file
  - **URI folder:** Points to specific folder
  - **ML Table:** Points to folder or file, including a schema to read as tabular data

> **Data Asset Management:** It is the **implementation** and **monitoring** of datastores and datasets. It is **version and tracking**. It is registering and **retrieveing those versions**. It is how we are monitoring datasets and how we look at **drift detection**. We can access datasets that Azure provides and look at public datasets

### <a href="/posts_blogs/blogs_dp100/createDatastoresDataAssets" style="color:skyblue;" rel="noopener">Create Datastores and Data Assets using Python SDK</a> (Imp)

> ### Mounting and Downloading Files for Datasets 

|                     | Mount Files                                         | Download Files                                        |
|---------------------|-----------------------------------------------------|-------------------------------------------------------|
| **Description**         | Files do not reside in compute                     | Files downloaded to compute                           |
| **Processing**          | More streaming - means more processing/moving of data| Less streaming - less processing as data has been downloaded |
| **Usage**               | Good if you don’t use all files from dataset       | Good if you use all files from dataset               |
| **Available for**       | Datasets created from ADLS, SQL, Database, PostgreSQL| Datasets created from ADLS, SQL, Database, PostgreSQL|

<!-- **In Azure ML Studio:**

- Create Datastore
```python
from azureml.core import Workspace, Datastore
ws = Workspace.get(name='DP100Testing', subscription_id='5fb9753f-1afe-4b8e-a65c-e402ecd9f0a5', resource_group='AZ800')
blob_ds = Datastore.register_azure_blob_container(
    workspace = WS, datastore_name='example',
    container_name='azureml',
    account_name='dp100testing1952224325',
    account_key='lxt3bpnDPOj zcN1081Pg38wMXI0QzRYlbbZnvghwGd82A518uj 5NTZGWZr7F/120jZvVuVGQ4XSk+ASt91vKog=='
)
```

- Print Datastore 
```python
for ds_name in ws.datastores:
    print(ds_name)
```

- Create dataset
```python
VERSION = "1"
my_data = Data(
    path = https://dp100testing1952224325.blob.core.windows.net/azureml/AdventureWorks Sales-2.xlsx,
    type = AssetTypes.URI_FILE,
    description = "Sales",
    name = "AW-Sales",
    version = VERSION
)
ml_client.data.create_or_update(my_data)
``` -->

> ### Preprocessing of Data 

**Steps** for Preprocessing of Data:
- **Data quality assessment**
- **Data Cleaning:** Look for **Missing data**, Noisy data
- **Data transformation:** **Aggregation**, **Feature selection & engineering**, **Normalization** (combining all the data so that everything is standardized), converting **categorical features** into numerical indicators, **dropping high cardinality** features like IDs
- **Data reduction:** employ math to filter out unnecessary data

> ### Feature Selection and Feature Engineering

- **Feature Selection:** The process of **selecting specific variables (features)** that contribute the most to the prediction variable in our algorithms.

- **Feature Engineering:** The process of **selecting and expressing data** in a way that improves the performance of machine learning models. 
    - **Wrapper method:** Wrapper methods **evaluate subsets of features** by training a model on them and assessing the model's performance. All **combinations** are evaluated and the **best one** is chosen. Prone to overfitting
    - **Filter method:** Filter methods **apply statistical techniques** to evaluate the relevance of each feature independently from the machine learning model. They **assign a score to each feature** based on various statistical tests and **rank them accordingly**. Thus **Features** can then be **included or removed** from the dataset **based on their scores**. This method is **relationship based**. Faster than wrapper method. 

> ### Differential Privacy: Eg of Responsible AI*

**Differential Privacy** seeks to **protect individual data by adding statistical noise to the analysis process**. **Minimizing** risk of **personal identification** and **ensuring data privacy.** It ensures that the **output** of a data analysis algorithm does **not reveal sensitive information** about any individual.
- When applying differential privacy, **Epsilon (ε), Bounds and Sample size** needs to be **defined**
- **Privacy Loss Paramter/Epsilon (ε)** is a key parameter in differential privacy that **controls the balance between privacy and accuracy - Value ranges between 0 and 1**
  - **Low Epsilon (ε)**: **High privacy** - **Low accuracy** - More noise - High data obscurity - Data more difficult to interpret
  - **High Epsilon (ε)**: **Low Privacy** - **High accuracy** - Less noise - Less data obscurity - Data more accurate and easier to interpret 

```python
# Sample code snippet
privacy_usage = { 'epsilon': 0.10},
                data_lower = lower_range[1],
                data_upper = upper_range[10],
                data_rows = sample
```

> ### Accessing Data During Interactive Development

**Data Wrangling:** It is the process of **transforming data** to a format that’s best suited to the needs of ML model. 

- Step 1: Open a notebook with a running ML Azure Kernel

- Step 2: Create **ML Client** and a **datastore** (code mentioned before)

- Step 3: Build a **URI** OR **Directly grab** it in the **Studio UI**

```python
# Azure Machine Learning workspace details:
subscription = '<subscription_id>'
resource_group = '<resource_group>'
workspace = '<workspace>'
datastore_name = '<datastore>'
path_on_datastore = '<path>'

# Long-form Datastore URI format:
uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}'
```

OR go to Data -> specific Datastore -> select csv file -> copy URI (Datastore URI or Storage URI)
  - **Note:** The **Datastore URI** is only **applicable to Azure ML** and the **Storage URI** is a more generic storage endpoint, which is used only **outside Azure ML**.

- Step 4: Load a **Pandas Dataframe**

```python
# Import pandas library
import pandas as pd

# Populate dataframe "my_dataframe" using the pandas read CSV method by passing in the URI acquired
my_dataframe = pd.read_csv("URI")

# Then run dataframe head passing in a value for the number of rows you want to return
my_dataframe.head(1000)
```

- Step 5: **Wrangle** - Replace **Missing** Strings

```python
# Fill missing values in the "Claim Network Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Claim Network Status": "Unkown"}, inplace=True)

# Fill missing values in the "Payment Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Payment Status": "Unkown"}, inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 6: **Wrangle** - **Delete Rows** With any **Empty** Columns

```python
# Drop rows with any missing values and update the dataframe in place
my_dataframe.dropna(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

> ### Wrangling Data with Apache Spark

- Step 1: Create a **compute** to power the notebook - Compute instance / Synapse Spark Pool / Azure ML Serverless Spark

- Step 2: Open a **notebook** and we use **Azure ML Serverless Spark** as compute for ease of use

- Step 3: Build a **URI** or grab it from the Studio UI

- Step 4: Load a **PySpark Pandas Dataframe**

```python
# Import pyspark pandas library
import pyspark.pandas as pd
my_dataframe = pd.read_csv("URI")
my_dataframe.head(1000)
```

- Step 5: Wrangle - Replace **Missing** Strings (code given above)

- Step 6: Wrangle - **Delete rows** with any **empty** columns (code given above)

- Step 7: **Wrangle** - Remove **Duplicate** Rows 

```python
# Drop duplicate rows and update the dataframe in place
my_dataframe.drop_duplicates(inplace=True)

# Sort the dataframe by its index and update the dataframe in place
my_dataframe.sort_index(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 8: **Save** the Transformed Data

```python
# Save the dataframe to a CSV file at the specified Azure Data Lake Storage path
my_dataframe.to_csv(
    "abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled"
    )
```

> ### <a href="/posts_blogs/blogs_dp100/Lab2WrangleData" style="color:skyblue;" rel="noopener">Wrangle Data with Python in Azure ML - Lab 2</a>

### (III) _<u>THREE WAYS TO BUILD AND TRAIN ML MODEL</u>_

> ### (A) Build & Train Models using <u>Azure ML Designer</u> 

- Step 1: Open **Azure ML Studio** -> Select **Designer**
- Step 2: Create new **pipeline (Classic Pre-built/Custom)**. Say we select a classic pre-built model for example - Regression Automobile Price prediction (Basic)
- Step 3: Now **run** this model by setting up a **pipeline job and a compute**
- Step 4: **Evaluate** the model based on metrics: **coefficient of determination, mean absolute error, relative absolute error, relative squared error, root mean squared error**

> **Dataset -> Select Columns in Dataset -> Clean Missing Data -> Split Data & (Linear Regression) -> Train Model -> Score Model -> Evaluate Model**

[![Screenshot-2024-05-16-at-1-06-58-AM.png](https://i.postimg.cc/26zyrWfp/Screenshot-2024-05-16-at-1-06-58-AM.png)](https://postimg.cc/B8V4CX6N)

> ### Custom Code Components

Custom code modules are created using Python. Supported libraries include NumPy, SciPy, scikit-learn, Theano, TensorFlow, Keras, PyTorch, pandas, and matplotlib

- **Create Custom Model:** Develop your custom model using **"Create Python Model"** module
- **Link** the custom model to the **"Train Model"** module to train it with your dataset
- Since standard evaluation modules are **not compatible**, use the **"Execute Python Script" module** to run evaluation scripts for the custom model

[![Screenshot-2024-05-16-at-1-07-53-AM.png](https://i.postimg.cc/nhyTRFf9/Screenshot-2024-05-16-at-1-07-53-AM.png)](https://postimg.cc/4Y1znTdX)
[![Screenshot-2024-05-16-at-1-08-23-AM.png](https://i.postimg.cc/Kj8dGGzv/Screenshot-2024-05-16-at-1-08-23-AM.png)](https://postimg.cc/7GjBXy0p)

> ### <a href="/posts_blogs/blogs_dp100/Lab3CreateBasicPipeline" style="color:skyblue;" rel="noopener">Create a basic pipeline in Azure ML Studio - Lab 3</a>

> ### Filter Based Feature Selection Module

- **Configuration Options:**
  - Operate on Feature Columns only - True/False
  - Number of desires features (Specify the number of features to output in the results): 1
  - Feature scoring method - **PearsonCorrelation/ChiSquared**
  - Target Column: Specify target column

### Pearson Correlation vs Chi-Squared Statistics

| Aspect                      | Pearson Correlation                                  | Chi-Squared Statistics                                    |
|-----------------------------|-----------------------------------------------------|----------------------------------------------------------|
| **Purpose**                 | Measures the strength and direction of a linear **relationship** between two quantitative variables | It is a comparative test that reveals **how close expected values are to actual results**. |
| **Correlation Coefficient (R)** | Ranges from -1 to +1: <br> - **0**: No correlation <br> - **+1**: Perfect positive correlation <br> - **-1**: Perfect negative correlation | **No correlation coefficient**; lower values indicate a better fit to expected values |
| **Type of Variables**       | **Quantitative**                                        | **Categorical**                                              |
| **Interpretation**          | - **Positive Correlation**: e.g., More rain increases humidity <br> - **Negative Correlation**: e.g., Higher altitude decreases temperature | **Indicates** whether a **relationship exists** but does **not specify the type** (positive or negative) |
| **Value Indication**        | Indicates the strength of the relationship          | Smaller values indicate a better fit and existence of a relationship |
| **Steps**                   | 1. Determine linearity <br> 2. Clean data <br> 3. Generate the coefficient <br> 4. Evaluate the results |  |

> ### Permutation Feature Importance Module

It refers to **randomly shuffling data while keeping everything else constant** and then seeing if we have a change in whatever feature column we were looking at. From that we can generate new prediction based on results. Compute feature importance score by calculating decrease in quality

- **Configuration Options:**
  - **Random seed** (Random number generator seed value - It is going to **randomize the data**) = 1023 (Say)
  - **Metric for measuring performance:**
    - **Classification metrics** - **Accuracy** (how accurate model is), **Precision** (how good model is), **Recall** (how many times model was able to detect a specific category)
    - **Regression metrics** - **Mean Absolute Error**, **Root Mean Squared Error**, **Relative Absolute Error**, **Relative Squared Error**, **Coefficient of Determination** (R squared)

> ### Applying Automated ML to Explore Models

Automated ML democratizes machine learning with a **no-code approach**, making it easy to explore optimal machine learning models. Automated ML handles **preprocessing, featurization, transformation, scaling,** and **normalization**. At the end, it **scores** the model by selecting a metric and **deploying** the model. Automated ML is used for exploring optimal algorithms and parameters to **solve** a particular problem **without a lot of human trial-and-error**.

- **Types of algorithm** in automated ML: **Classification**, **Regression** and **Time Series Forecasting** 

**Examples of Automated ML**:
- Tabular Data
- Computer Vision
- Natural Language Processing

> ### **Configure and submit Auto ML experiment using Python SDK*** (Imp)

- Step 1: **Create ML Client**

- Step 2: **Define MLTable**: (ML Table is already inputted)

- Step 3: **Define the AutoML Job**

```python
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml import automl, Input

# Create an Input object for the training data
my_training_input = Input(
    type=AssetTypes.MLTABLE, 
    path="./data/training-mltable-folder"
)

# Configure the classification job
my_classification_job = automl.classification( # for classfification, automl.classfication function is used
    compute="aemcmlcompute",
    experiment_name="this_experiment",
    training_data=my_training_input, # my_training_input refers to MLTable data asset created in Azure ML workpsace
    target_column_name="try_me",
    primary_metric="accuracy",
    n_cross_validations=4,
    enable_model_explainability=True,
    tags={"my_tag": "My value"}
)

# Specify Primary Metric (Important to specify)
# The primary metric is the target performance metric for which the optimal model will be determined. 
# Set the primary metric to the performance score for which you want to optimize the model.

# To retrieve the list of ClassificationPrimaryMetrics available
from azure.ai.ml.automl import ClassificationPrimaryMetrics
list(ClassificationPrimaryMetrics) 

# Set optional limits to minimize cost and time spent on training by set_limits()
my_classification_job.set_limits(
    timeout_minutes=600, # timeout_minutes: Number of minutes after which the complete AutoML experiment is terminated.
    trial_timeout_minutes=20, # trial_timeout_minutes: Maximum number of minutes one trial can take.
    max_trials=5, # max_trials: Maximum number of trials, or models that will be trained.
    enable_early_termination=True # enable_early_termination: Whether to end the experiment if the score isn't improving in the short term.
)
# Note: Can use multiple trials in parallel to save time. If using compute cluster, can have as many parallel trials as the max number of nodes
# Note: If you want to set max number of trials to be less than max number of nodes, use max_concurrent_trials

# Set optional training properties
my_classification_job.set_training(
    blocked_training_algorithms=["logistic_regression"], 
    enable_onnx_compatible_models=True
)
```

- Step 4: **Submit & Monitor Auto ML Job run**

```python
# Submit the AutoML job
returned_job = ml_client.jobs.create_or_update(
    my_classification_job
)
returned_job.services ["Studio"].endpoint

# Monitor Auto ML job runs 
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

**NOTE**: Set the **featurization mode to "off"** if you don’t want the automated machine learning to make changes to the data (no preprocessing of data). ALSO The **target** is the **column** you want **to predict**. The **primary metric** is the **performance score** for which you want to **optimize the model**. Set the **primary_metric to 'AUC_weighted'** to **optimize for the AUC_weighted metric**.

> ### <a href="/posts_blogs/blogs_dp100/Lab4AzureAutomatedML" style="color:skyblue;" rel="noopener">(B) Build & Train Models using <u>Azure Automated Machine Learning</u> - Lab 4</a>*

> ### <a href="/posts_blogs/blogs_dp100/TrainModelUsingPythonSDK" style="color:skyblue;" rel="noopener">(C) Training Model by using Python SDK</a>*

[![Screenshot-2024-05-17-at-2-06-10-AM.png](https://i.postimg.cc/5NPfWhNd/Screenshot-2024-05-17-at-2-06-10-AM.png)](https://postimg.cc/kBt0xhbf)

> ### Tuning Hyperparameters in Azure Machine Learning*

- **Parameters** are values **determined** from the **training features** as parameters. **Hyperparameter** are values used to **configure training behavior** but which are **not derived** from **training data**. **Hyperparameters**: **Top-level settings** you configure before running the ML algorithm, such as: train-test ratios, number of epochs, batch size. 

### _**Types of Hyperparameters**_*

  > **Discrete Hyperparameters:** 

  - Specified as **choice** among **discrete values** (**explicit** values/**finite** set of possibilties) which can be defined using:
    - **python list** _(Choice(values=[10,20,30]))_
    - **range** _(Choice(values=range(1,10)))_
    - **arbitrary set** of **comma-separated** values _(Choice(values=(30,50,100)))_

```python
{
    "batch_size": choice(1, 2, 3, 4)
    "number_of_hidden_layers": choice(range(1,5))
}
```

  | **Distribution Type**   | **Description**                                                                                                         |
|-------------------------|-------------------------------------------------------------------------------------------------------------------------|
| **Uniform(min_value, max_value)** | Returns a value uniformly distributed between min_value and max_value                                           |
| **LogUniform(min_value, max_value)** | Returns a value drawn according to exp(Uniform(min_value, max_value)) so that the logarithm of the return value is uniformly distributed |
| **Normal(mu, sigma)** | Returns a real value that's normally distributed with mean mu and standard deviation sigma                                |
| **LogNormal(mu, sigma)** | Returns a value drawn according to exp(Normal(mu, sigma)) so that the logarithm of the return value is normally distributed|

  > **Continuous Hyperparameters:** 

  - Specified from a **continuous (sliding) range** of values (**infinite** number of possibilities). 

```python
{
    "learning_rate": normal(10,3),
    "keep_probability":uniform(0.05, 0.1)
}
```

| **Distribution Type**            | **Description**                                                                                                         |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| **Uniform(min_value, max_value)** | Returns a value uniformly distributed between min_value and max_value                                                    |
| **LogUniform(min_value, max_value)** | Returns a value drawn according to exp(Uniform(min_value, max_value)) so that the logarithm of the return value is uniformly distributed |
| **Normal(mu, sigma)**            | Returns a real value that's normally distributed with mean mu and standard deviation sigma                               |
| **LogNormal(mu, sigma)**         | Returns a value drawn according to exp(Normal(mu, sigma)) so that the logarithm of the return value is normally distributed|

- **Hyperparameter tuning:** Process of finding the **configuration of Hyperparameters** that results in **determining performance metric** for which you want to optimize and selecting the **best performance model** 

  - **Search Space:** The set of **hyperparameter values** **tried** during hyperparameter **tuning** is known as the **search space**. E.g. sample search space indicates _**batch_size**_ hyperparameter and _**learning_rate**_ hyperparameter with value of mean = 10 and s.d = 3

```python
from azure.ai.ml.sweep import Choice, Normal

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),    
    learning_rate=Normal(mu=10, sigma=3),
)
```

- There are different **types of jobs** depending on how you want to execute a workload:
  - **Command:** Execute/**run a single script**.
  - **Sweep Job:** Perform **hyperparameter tuning** when executing a **single script**. Helps you **automate choosing** these **parameters**
  - **Pipeline:** **Run a pipeline** consisting of **multiple scripts** or components.

- **Note:** When you submit a **pipeline** you **created with designer** it will run as a **pipeline job**. When you submit an **Automated ML experiment**, it will also **run as a job**.

- **Sampling Methods** <a href="/posts_blogs/blogs_dp100/SamplingMethodsCodes" style="color:skyblue;" rel="noopener">**(Also codes)**</a> IMP*

[![Screenshot-2024-05-16-at-10-45-10-PM.png](https://i.postimg.cc/0yVpZMpP/Screenshot-2024-05-16-at-10-45-10-PM.png)](https://postimg.cc/RWn6Z0sD)

- **_<u>Define Primary Metric</u>_**

  - You can **define the objective** of your **sweep job** by specifying the primary metric and **goal** you want hyperparameter tuning to optimize. 
  - Each training job is evaluated for the primary metric.

    - **_primary_metric:_** The name of the primary metric and the name of the metric logged by the training script should be an exact match.
    - **_goal:_** It can be either **Maximize** or **Minimize**. This determines if the primary metric will be maximized or minimized during job evaluation.

```python
from azure.ai.ml.sweep import Uniform, Choice

command_job_for_sweep = command_job(
    learning_rate=Uniform ( min_value=0.05, max_value=0.1), 
                            batch_size = Choice (values = [16, 32, 64, 128]),
                            # Discrete hyperparameter used here
)

sweep_job = command_job_for_sweep.sweep(
    compute="cpu-cluster",
    sampling_algorithm = "bayesian", 
    primary_metric="accuracy",
    goal="Maximize",
)
```

> ### Early Termination Policies*

**Early Termination Policies:** Strategy used to **halt** the **training process** of new models when they **do not** yield significantly **better results** **compared to previously** trained models. **Increase efficiency** and **reduce costs** by stopping underperforming runs early. Generally **used** when working with **continuous hyperparameters** and **random** or **Bayesian sampling** method. 

- **Two Parameters** when using early termination policy: 
  - _**evaluation_interval:**_ Specifies at which interval you want the policy to be evaluated. Every time the primary metric is logged for a trial counts as an interval.
  - _**delay_evaluations**_:  Specifies when to start evaluating the policy. This parameter allows for at least a minimum of trials to complete without an early termination policy affecting them.

> **_<u>Bandit Policy</u>_**: 
  - Terminates when **primary metric** is **not within** a specified **slack_factor** (relative)/**slack amount** (absolute) 
  - **Stops runs** that perform **worse than the best run by a specified margin**

```python
from azure.ai.ml.sweep import BanditPolicy
  
sweep_job.early_termination = BanditPolicy(
    slack_amount = 0.2, 
    delay_evaluation = 5, 
    evaluation_interval = 1
)
# When after first five trials, best performing model has accuracy say 0.9.
# Thus all new models must perform better than 0.9-slack_amount = 0.9-0.2 = 0.7, the sweep job will continue.  
# If the new model has an accuracy score lower than 0.7, the policy will terminate the sweep job.
```

> **_<u>Median Stopping Policy</u>_**: 
  -  Terminates when **primary metric** value is **worse than median of averages** for all trials.

```python
from azure.ai.ml.sweep import MedianStoppingPolicy
  
sweep_job.early_termination = MedianStoppingPolicy(
    delay_evaluation = 5, 
    evaluation_interval = 1
)
# Say median accuracy score is 0.82 so far. 
# Any new model should have accuracy higher than 0.82 for the sweep job to continue. 
# If the new model has an accuracy score lower than 0.82, the policy will terminate the sweep job.
```

> **_<u>Truncation Selection Policy</u>_**: 
  - **Terminates/Cancels lowest performing** X% of trials at **each evaluation interval** based on the truncation_percentage value you specify for X.

```python
from azure.ai.ml.sweep import TruncationSelectionPolicy
  
sweep_job.early_termination = TruncationSelectionPolicy(
    evaluation_interval=1, 
    truncation_percentage=20, 
    delay_evaluation=4 
)
# For example, if the primary metric is accuracy, the job continues only if a new model's accuracy is not in the worst 20% of all trials so far. 
# If the fifth trial has the lowest accuracy among all trials, the sweep job stops.
```

> ### <a href="/posts_blogs/blogs_dp100/SweepJobforHyperparameterTuning" style="color:skyblue;" rel="noopener">Use a sweep job for hyperparameter tuning</a>*

> ### <a href="/posts_blogs/blogs_dp100/OptimizeHyperparameters" style="color:skyblue;" rel="noopener">Steps to Optimize Hyperparameters</a>

<!-- RUNNING MODEL TRAINING SCRIPTS -->

- **Notebooks v/s Scripts** 

| Notebooks                                   | Scripts                                |
|---------------------------------------------|----------------------------------------|
| Designed for interactive training           | Intended for automated training        |
| Contain other content                       | Stripped down to (mostly) code         |
| Contain alternative model ideas             | Run settled models                     |
| On-demand code (run specific cells as needed)| On-command functions (execute without supervision) |
| Awkward to deploy in CI/CD                  | Enable CI/CD processes                 |

> ### <a href="/posts_blogs/blogs_dp100/RunScriptInCommandJob" style="color:skyblue;" rel="noopener">Run Training Script as Command Job in Azure Ml</a>*
- Step 1: Convert Notebook to Script
- Step 2: Run a Script as a Command Job
- Step 3: Use Parameters in Command Job

> ### Responsible AI Dashboard & Evaluate Automated ML run including Responsible AI*

- **Responsible AI Dashboard**

| Model debugging           | Business decision making   |
|---------------------------|----------------------------|
| Error analysis            | Causal analysis            |
| Data explorer             | Counterfactual what-if     |
| Model overview            |                            |
| Fairness assessment       |                            |
| Model interpretability    |                            |
| Counterfactual what-if    |                            |

[![Screenshot-2024-05-17-at-2-49-51-AM.png](https://i.postimg.cc/vZxqQKwS/Screenshot-2024-05-17-at-2-49-51-AM.png)](https://postimg.cc/jDKQcZ5z)

[![Screenshot-2024-05-30-at-1-51-40-AM.png](https://i.postimg.cc/gjLqM0gk/Screenshot-2024-05-30-at-1-51-40-AM.png)](https://postimg.cc/tnyxJ90L)


## Run Pipelines in Azure ML

> ### _**<u>Create Components</u>**_

**Components:** Allow to **create reusable scripts** that can be **easily shared** across users within the same ML workspace. Use components to **build ML pipeline**.

- Component consist of **3 parts:**
  - **Metadata:** Includes the component's name, version, etc.
  - **Interface:** Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts).
  - **Command**, **code** and **environment:** Specifies how to run the code.

- To **create component**, you need **2 files:**
  - **Script** that contains workflow u want to execute
  - **YAML file** to define metadata, interface, command, code and environment of the component/ or use command_component() function to create YAML file

> ### <a href="/posts_blogs/blogs_dp100/CreateComponent" style="color:skyblue;" rel="noopener">Sample code to create a Component</a>*
- To **create components** in **pipeline**, create a **script** and **YAML file**
- **Load the component**
- To make **components accessible** to other users in workspace, **register components** to Azure ML workspace

> ### _**<u>Create Pipelines</u>**_

**Pipeline** is a **workflow** of **ML tasks** in which each task is defined as a component. A pipeline can be **executed** as a **process** by running the pipeline as a **pipeline job**. Each component is executed as a child job as part of the overall pipeline job.

> ### <a href="/posts_blogs/blogs_dp100/BuildPipeline" style="color:skyblue;" rel="noopener">Build a Pipeline and Run a Pipeline Job</a>*

> ### Track Model with MLFlow in Jobs*

(I) _**Introduction**_

**MLFlow** is an **open-source platform** designed to streamline the **tracking of model metrics and artifacts**, such as experiments and models. It is **cloud-agnostic** and **enables continuous integration** **without** necessitating **changes** to existing training routines. Install it using _**pip install mlflow azure-mlflow**_. **Mlflow model** can **opt** for **no-code deployment** in Azure ML. Mlflow standardizes packaging of models, which means **model** can **easily** be **imported or exported** across different workflows.

(II) _**TRACK METRICS WITH MLFLOW**_

**Two methods** to **track ML jobs** with **Mlflow**:
- **Enable autologging** using _**mlflow.autolog()**_
- Use **logging functions** to track **custom metrics** using mlflow

> #### Enable autologging

- **Automatically logs parameters**, **metrics**, and **model artifacts** **without** anyone **needing** to **specify** what needs to be **logged**.
- Supported by
    • **Scikit-learn**
    • **TensorFlow** and **Keras**
    • **XGBoost**
    • **Light GBM**
    • **Spark**
    • **Fastai**
    • **Pytorch**

- **To enable autologging**, add following code for training script

```python
import mlflow

mlflow.autolog()
```
  
> #### Log Metrics with Mlflow i.e. Use logging functions to track custom metrics using mlflow
  
- Can decide whatever custom metric u want to log with MLflow

- _**mlflow.log_param(my_custom_param)**_: **Log single key-value parameter**. Use this function for an **input parameter** you want **to** **log**.
- _**mlflow.log_metric(my_custom_metric)**_: Log **single key-value metric**. Value must be a **number**. Use this function for **any output** you want to **store** with the run.
- _**mlflow.log_artifact(my_custom_artifact)**_: **Log a file**. Use this function for **any plot** you want to log, **save as image file first**.

- **Sample code to add Mlflow** to an existing training script:

```python
import mlflow
  
reg_rate = 0.1
mlflow.log_param("Regularization rate", reg_rate)
```

- Finally **submit** the job

(III) _**VIEW METRICS AND EVALUATE MODELS**_

When **job is complete**, can **explore and evaluate** models:
  - **Review metrics in Azure ML Studio**
  - **Retrieve metrics with Mlflow in a notebook**
  - **Retrieve runs**

> #### Review metrics in Azure ML Studio

Open Studio and select your experiment
- In **Details tab**, all **logged parameters** are **shown under** _**Params**_
- Select the **Metrics tab** and select the **metric** you **want to explore**.
- Any **plots** that are **logged** as artifacts can be **found under _Images_**.
- The **model assets** that can be **used to register and deploy** the model are **stored** in the **models folder under _Outputs + logs_**.
  
> #### Retrieve metrics with Mlflow in notebook
  
- **Search all active experiments** in workspace using Mlflow

```python
experiments = mlflow.search_experiments(max_results=2)
for exp in experiments:
    print(exp.name)
```
  
- **Retrieve archived experiments too**, then include the option _**ViewType.ALL**_

```python
from mlflow.entities import ViewType

experiments = mlflow.search_experiments(view_type=ViewType.ALL)
for exp in experiments:
    print(exp.name)
```
  
- **Retrieve a specific experiment**

```python
exp = mlflow.get_experiment_by_name(experiment_name)
print(exp)
```
  
> #### Retrieve runs

- To search for runs, you **need** either **expt ID or expt name**. E.g. retrieve metrics of a specific run:
```python
mlflow.search_runs(exp.experiment_id)
```
  
- Can also **search across** **all experiments** in workspace using _**search_all_experiments=True**_
  
- By **default**, experiments are **ordered descending** by _**start_time**_, which is **time** the **expt** **was queued in azure ML**. However, you can **change this** default by using the **parameter _order_by_**.

- For example, if you want to **sort by start time** and only **show** the **last two results**:
```python
mlflow.search_runs(exp.experiment_id, order_by=["start_time DESC"], max_results=2)
```
  
- You can also **look for run** with a **specific combination** in the hyperparameters:
```python
mlflow.search_runs(
    exp.experiment_id, filter_string="params.num_boost_round='100'", max_results=2
)
```

> ### Register an Mlflow model in Azure Machine Learning*

When you **train** and **log** a model, you **store** all **relevant artifacts in a directory**. When you register the model, an _**MLmodel**_ file is created in that directory. The _**Mlmodel**_ file contains the models metadata, which allow for model traceability. The model is logged with **.fit() method**

- The **framework used to train** model is identified and included as the _**flavor**_ of your model
- Some **common flavors** that you can use with autologging are: (**Syntax: mlflow.flavor.autolog**)
  - Keras: _**mlflow.keras.autolog**_
  - Scikit-learn: _**mlflow.sklearn.autolog()**_
  - LightGBM: _**mlflow.lightgbm.autolog**_
  - XGBoost: _**mlflow.xgboost.autolog**_
  - TensorFlow: _**mlflow.tensorflow.autolog**_
  - PyTorch: _**mlflow.pytorch.autolog**_
  - ONNX: _**mlflow.onnx.autolog**_

> #### Customize the model signature

- **Model signature** defines the **schema** of **model's inputs and outputs**. The signature is stored in **JSON format** in Mlmodel file, with other metadata of model.
- The model signature can be **inferred from dataset or created manually** by hand. 

- Sample code of **model signature inferred from dataset**

```python
import pandas as pd
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature

iris = datasets.load_iris()
iris_train = pd.DataFrame(iris.data, columns=iris.feature_names)
clf = RandomForestClassifier(max_depth=7, random_state=0)
clf.fit(iris_train, iris.target)

# Infer the signature from the training dataset and model's predictions
signature = infer_signature(iris_train, clf.predict(iris_train))

# Log the scikit-learn model with the custom signature
mlflow.sklearn.log_model(clf, "iris_rf", signature=signature)
```

- Sample code of **creating signatures manually**

```python
from mlflow.models.signature import ModelSignature
from mlflow.types.schema import Schema, ColSpec

# Define the schema for the input data
input_schema = Schema([
  ColSpec("double", "sepal length (cm)"),
  ColSpec("double", "sepal width (cm)"),
  ColSpec("double", "petal length (cm)"),
  ColSpec("double", "petal width (cm)"),
])

# Define the schema for the output data
output_schema = Schema([ColSpec("long")])

# Create the signature object
signature = ModelSignature(inputs=input_schema, outputs=output_schema)
```

> #### Understand the Mlflow model format

Mlflow uses _**MLModel format**_ to **store all** relevant model assets **in folder or directory**. _**MLmodel file**_ is one of the files in the directory. 

- **Mlmodel file format may include**

```yml
artifact_path: classifier # During the training job, the model is logged to this path.
flavors: # The machine learning library with which the model was created.
  fastai:
    data: model.fastai
    fastai_version: 2.4.1
  python_function:
    data: model.fastai
    env: conda.yaml
    loader_module: mlflow.fastai
    python_version: 3.8.12
model_uuid: e694c68eba484299976b06ab9058f636 # The unique identifier of the registered model.
run_id: e13da8ac-b1e6-45d4-a9b2-6a0a5cfac537 # The unique identifier of job run during which the model was created.
signature: # Specifies the schema of the model's inputs and outputs:
# inputs: Valid input to the model. For example, a subset of the training dataset.
# outputs: Valid model output. For example, model predictions for the input dataset.
  inputs: '[{"type": "tensor",
             "tensor-spec": 
                 {"dtype": "uint8", "shape": [-1, 300, 300, 3]}
           }]'
  outputs: '[{"type": "tensor", 
              "tensor-spec": 
                 {"dtype": "float32", "shape": [-1,2]}
            }]'
```

> #### **Choose the flavor**

A **flavor** is the machine learning **library** with which the **model was created**. _**Python function flavor**_ is the **default model interface** for models created from an MLflow run. Any MLflow python model can be loaded as a _**python_function model**_, which allows for workflows like deployment to work with any python model regardless of which framework was used to produce the model. 

```yml
artifact_path: pipeline
flavors:
  python_function:
    env:
      conda: conda.yaml
      virtualenv: python_env.yaml
    loader_module: mlflow.sklearn
    model_path: model.pkl
    predict_fn: predict
    python_version: 3.8.5
  sklearn:
    code: null
    pickled_model: model.pkl
    serialization_format: cloudpickle
    sklearn_version: 1.2.0
mlflow_version: 2.1.0
model_uuid: b8f9fe56972e48f2b8c958a3afb9c85d
run_id: 596d2e7a-c7ed-4596-a4d2-a30755c0bfa5
signature:
  inputs: '[{"name": "age", "type": "long"}, {"name": "sex", "type": "long"}, {"name":
    "cp", "type": "long"}, {"name": "trestbps", "type": "long"}, {"name": "chol",
    "type": "long"}, {"name": "fbs", "type": "long"}, {"name": "restecg", "type":
    "long"}, {"name": "thalach", "type": "long"}, {"name": "exang", "type": "long"},
    {"name": "oldpeak", "type": "double"}, {"name": "slope", "type": "long"}, {"name":
    "ca", "type": "long"}, {"name": "thal", "type": "string"}]'
  outputs: '[{"name": "target", "type": "long"}]'
```

- _**Configure the signature**_

There are **two types of signatures:**
  - **Column-based:** used for **tabular data** with a pandas. **Dataframe as inputs**.
  - **Tensor-based:** used for **n-dimensional arrays** or **tensors** (often used for unstructured data like text or images), with **numpy.ndarray** **as inputs**.

> #### Register an Mlflow model

To **easily manage model**, **store** model in Azure ML model **registry**. It makes it **easy to organize** and **keep track** of trained models. When you register a model, you **store and version** the model in workspace. The **registered models** are **identified by name** and **version**. You **can also** register models trained **outside Azure ML** providing the local path to the models artifacts. 

**3 types of models you can register:**
  - **MLflow:** Model trained and tracked with MLflow. Recommended for **standard deployments**.
  - **Custom:** Model type with a custom standard not currently supported by Azure Machine Learning.
  - **Triton:** Model type for **deep learning workloads**. Commonly used for **TensorFlow** and **PyTorch model** deployments. Ideal for **compute-intensive and no-code deployments**

- **To register** an Mlflow model, you can use **Studio**, **Azure CLI**, or **Python SDK**

- **Sample code of training model by submitting script as command job**

```python
from azure.ai.ml import command

# configure job

job = command(
    code="./src",
    command="python train-model-signature.py --training_data diabetes.csv",
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    display_name="diabetes-train-signature",
    experiment_name="diabetes-training"
    )

# submit job
returned_job = ml_client.create_or_update(job)
aml_url = returned_job.studio_url
print("Monitor your job at", aml_url)
```

- **Once job is completed and model is trained, use job name to find job run and register the model from its outputs**

```python
from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

job_name = returned_job.name

run_model = Model(
    path=f"azureml://jobs/{job_name}/outputs/artifacts/paths/model/",
    name="mlflow-diabetes",
    description="Model created from run.",
    type=AssetTypes.MLFLOW_MODEL,
)
# Uncomment after adding required details above
ml_client.models.create_or_update(run_model)
```
All **registered models** listed in _**Models page**_ of Azure **ML studio** 








