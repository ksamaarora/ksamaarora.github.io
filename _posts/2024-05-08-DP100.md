---
layout: post
title:  "Designing and Implementing Data Science Solution on Azure (DP-100)"
date:   2024-05-08 09:29:20 +0700
tags:
  - cloud
categories: jekyll update
usemathjax: true
---

> ### <a href="/posts_blogs/blogs_dp100/datasciencelifecycle" style="color:skyblue;" rel="noopener">Data Science Life Cycle & Responsible AI Guidelines</a>

> ### Azure Machine Learning Model 

[![Screenshot-2024-05-12-at-3-53-44-PM.png](https://i.postimg.cc/mkC3Mbcq/Screenshot-2024-05-12-at-3-53-44-PM.png)](https://postimg.cc/p5VnMMJY)

> ### Resources and Assets 

[![Screenshot-2024-05-12-at-5-50-16-PM.png](https://i.postimg.cc/N0DZPCqJ/Screenshot-2024-05-12-at-5-50-16-PM.png)](https://postimg.cc/7CCB5VL0)

**Resources** provide the **infrastructure and services** to build solutions, and **assets** are the **products or content created or configured** by data scientists and data engineers.
Examples of resources include compute and datastore, and examples of assets are machine learning models and data assets.

- **Application Insights, Azure Key Vault, and Azure Storage account** are resources are **provisioned by default** when you **deploy** an Azure Machine Learning **workspace**. 

  - **Application Insights** is an extension of Azure Monitor and provides performance monitoring for several types of applications, including machine learning solutions. 
  - The **key vault** is used to store secrets required to connect to data and other resources and assets. 
  - The **storage account** is used to store machine learning notebooks and logs related to machine learning jobs and other activities.

- A **compute target** is a designated **compute resource** or **environment** where you run your training script or host your service deployment. You **can use** a **different compute target** for each phase of your project. For example, you might use a compute instance to train your model and Kubernetes for a deployed model. Compute targets can be defined for **training as well as production phases** of your Azure Machine Learning projects.

> ### Introducing Essential of Python SDK

The basic pattern used in managing Azure Machine Learning resources using the Azure Machine Learning SDK for Python starts with **importing any required libraries and classes**, then **defining the attributes and configuration** of the resource, and then executing a **create or update method**.

- **Authenticate ML Client**: From the Azure Machine Learning SDK for Python, MLClient is **used to connect to the workspace**. It is also used to execute methods that create or update resources, such as datastores and compute.
```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
```

- **Create New Machine Learning Workspace**
```python
from azureml.core import Workspace
my_ws = Workspace(
    name="another-workspace",
    location="westus",
    display_name="another-workspace for you to view", 
    description="This is just an example"
)
ml_client.workspaces.begin_create(my_ws)
```

- **Create a new Compute Targets**: Compute Targets are for production workloads and Compute Instances are used for powering notebooks 
```python
from azureml.core import ComputeTarget, AmlCompute
my_little_cluster = AmlCompute(
    name="basic-cluster",
    type="amlcompute",
    size="STANDARD_DS3_v2",
    location="westus",
    min_instances=0,
    max_instances=2,
    idle_time_before_scale_down=60,
)
ml_client.begin_create_or_update (my_little_cluster)
```

- **Create a new Datastore**
```python
from azureml.core import Datastore, AzureBlobDatastore 
a_blob_store= AzureBlobDatastore(
    name="my_blob_example",
    description="Datastore pointing to a blob container.",
    account_name="myexistingblobstore", 
    container_name="stuffandsuch-container",
    credentials={
        "account_key": "XXXXYYYXXXzzzYYYZZZxxx"
    },
)
ml_client.create_or_update(a_blob_store)
```

- **Create List Environments**
```python
envs = ml_client.environments.list()
for env in envs:
    print(env.name)
```

- **Create a Custom Environment** 
```python
from azureml.core import Environment
my_env = Environment(
    image="pytorch/pytorch: latest", # base image to use
    name="docker-image-example", # name of the model
    description="Environment created from a Docker image.",
)
ml_client.environments.create_or_update(my_env) # use the MLClient to connect to workspace and create/register the environment
```

> ### Compute

| **Compute Type** | **Description** |
|---|---|
| **Compute Instances** | The engine that powers your machine learning experiments. Used for everyday starting builds. Just like a Virtual Machine. |
| **Compute Clusters** | Think of compute clusters as a bunch of workers. We can scale our work using compute clusters as we can increase the number of nodes (workers) used for a particular task/ workload. Used for large runs. |
| | - **Dedicated Cluster:** These are just ready when you are - more expensive |
| | - **Low-Priority Cluster:** These systems are going to be ready when you are, probably. Low priority clusters may be accessed by multiple users thus you may get access to it but just in a few minutes or so - less expensive |
| **Inference Cluster/Kubernetes Cluster** | We will be using Azure Kubernetes or AksCompute Cluster. This will be useful for Production environments, Multi-cloud environments, On-premises |
| **Attached Compute** | "Catch-All". Used for Specialized needs. This is useful for HDInsight cluster, Virtual Machine, Databricks cluster, Data Lake Analytics, Azure Synapse Spark Pool |

<!-- Compute Instances 
[![Screenshot-2024-05-13-at-3-16-08-AM.png](https://i.postimg.cc/KjXNVNBj/Screenshot-2024-05-13-at-3-16-08-AM.png)](https://postimg.cc/mzwC17dW)

Compute Cluster
[![Screenshot-2024-05-13-at-3-17-46-AM.png](https://i.postimg.cc/LXW7sLfF/Screenshot-2024-05-13-at-3-17-46-AM.png)](https://postimg.cc/62dzj7YM)

[![Screenshot-2024-05-13-at-3-18-26-AM.png](https://i.postimg.cc/pd44WKKb/Screenshot-2024-05-13-at-3-18-26-AM.png)](https://postimg.cc/Y4Nb8GXb)

Interference Cluster 
[![Screenshot-2024-05-13-at-3-20-09-AM.png](https://i.postimg.cc/dtf8SnFY/Screenshot-2024-05-13-at-3-20-09-AM.png)](https://postimg.cc/crQgvR69)

Attached Compute
[![Screenshot-2024-05-13-at-3-21-09-AM.png](https://i.postimg.cc/yWC7kw9K/Screenshot-2024-05-13-at-3-21-09-AM.png)](https://postimg.cc/p5CwScqc) -->


> ### Apache Spark Tools as Compute Targets

**Azure Synapse:** It is an enterprise analytics service platform that enables data warehousing, analytics, processing and integration and pipeline framed with a massively parallel processing architecture. Synapse supports bot SQL and Spark technologies. 

**Azure Synapse Spark Pools:**

- When setting up a Synapse Spark pool as an attached compute target in Azure Machine Learning studio:
Select an existing Azure Synapse workspace and an existing Spark pool in that workspace -> tick the option to set up a **managed identity** -> Choose system-assigned or user-assigned -> To **reliably connect to your new compute resource** to run workloads, Go to synapse studio and **assign managed identity in Azure ML to role of Synapse Administrator**. 

- **Serverless Spark Pools** can be used as a form of compute to set up notebooks in Azure ML Studio

> ### Selecting development approaches to build or train models

Three ways to interact with Azure Machine Learning
  - **Azure CLI:** Following command is used to define an compute resource and then create a Azure ML Workspace
    - ```python
      az ml compute create -f create-cluster.yml
      ```
  - **Python SDK:** Previously mentioned codes are used for developing and training models 
  - **Azure ML Studio:** used for low code training and development, data management and monitoring
  - **Git:** Azure ML Workspaces work indirectly with Git for **source control**, and you can use a local Git repository by cloning it
    - ```python
     git clone url_of_repository
     ```
  - **Environments:** Can be another form of **source control** and multiple custom environments can be created or curated environments can be used.

**Note:** Azure CLI, AML studio, and Azure Machine Learning SDK for Python are all commonly used in at least one phase of building, training, and deploying models in Azure Machine Learning. 

[![Screenshot-2024-05-13-at-3-31-18-AM.png](https://i.postimg.cc/2SHqc7ZD/Screenshot-2024-05-13-at-3-31-18-AM.png)](https://postimg.cc/hXdPhxsy)

> #### Azure Machine Learning Environments

**Environment:** Contains python packages, environment variables, software settings, runtimes. Environment ensures version control, reproducible and auditable. 

- **Types of Environment:**
  - **Curated Environment:** Microsoft is responsible for everything
  - **User-Managed Environment:** You are responsible for everything
  - **System Managed:** Conda is responsible for everything

> ### Attached Compute - HD Insights and Apache Spark

```python
from azureml.core import RemoteCompute, ComputeTarget
# Ubuntu VMs only
# VM must have public IP addy
my_resource_id = "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>"
my_compute_target_name = "attached_existingVM"
attached_target_config = RemoteCompute.attach
#
attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',
                                                    ssh_port=22,
                                                    username= '<username>'
                                                    password="<password>")
# Attach the compute
compute = ComputeTarget.attach(my_ws, my_compute_target_name, attached_target_config)
compute.wait_for_completion (show_output=True)
```

**Note:** When considering attaching an existing virtual machine to your Azure ML workspace as a compute target, it's crucial that the **external Vms** must be **Ubuntu only** and must have an **public IP address only**.  
However, the primary reason for **choosing an existing VM** over a new compute instance is to **utilize unused capacity effectively**.

```python
from azureml.core import ScriptRunConfig
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Create environment
my_env = Environment(name="mycoenv")

# Dependencies
my_env.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])

# Base Image. Leave out to use the default image: "azureml.core.runconfig.DEFAULT_CPU_IMAGE"

# Configure with the existing VM as the compute target, using the environment we just defined 
src = ScriptRunConfig(source_directory=".", script="train.py", compute_target=compute, environment=my_env)
```

> ### <a href="/posts_blogs/blogs_dp100/Lab1CreateMLWorkspace" style="color:skyblue;" rel="noopener">Create Azure Machine Learning Workspace - Lab 1</a>

> ### Managing and Exploring Data Assets 

- **Datastores:** Datastores are **reference** to existing **Azure Storage resource** and are used to **read data directly** from that source
- **Datasets:** Datasets are also **references** but are ****further abstracted** to point to **specific versioned** sets of data. **Valid asset types** when setting up an Azure ML dataset is are **tables and folders**
- **Data Asset Management:** It is the implementation and monitoring of datastores and datasets. It is version and tracking. It is registering and retrieveing those versions. It is how we are monitoring datasets and how we look at drift detection. We can access datasets that Azure provides and look at public datasets.

**SDK:**
[![Screenshot-2024-05-14-at-2-45-42-AM.png](https://i.postimg.cc/1Xkc4wC7/Screenshot-2024-05-14-at-2-45-42-AM.png)](https://postimg.cc/wRksGydD)

**Note:** When working with the SDK, **import** the requires modules, followed by **defining 'my_path'** which indicates the **location of the datastore** (e.g., blob storage account). Next, **define 'my_data'** to specify the **location of the file or table** within the datastore. Finally, utilize the **'create' or 'update' function** to manipulate 'my_data' accordingly.

> ### Mounting and Downloading Files for Datasets 

|                     | Mount Files                                         | Download Files                                        |
|---------------------|-----------------------------------------------------|-------------------------------------------------------|
| **Description**         | Files do not reside in compute                     | Files downloaded to compute                           |
| **Processing**          | More streaming - means more processing/moving of data| Less streaming - less processing as data has been downloaded |
| **Usage**               | Good if you don’t use all files from dataset       | Good if you use all files from dataset               |
| **Available for**       | Datasets created from ADLS, SQL, Database, PostgreSQL| Datasets created from ADLS, SQL, Database, PostgreSQL|

- Managing Data Assets Using the SDK

[![Screenshot-2024-05-14-at-3-02-55-AM.png](https://i.postimg.cc/rpp9zVy4/Screenshot-2024-05-14-at-3-02-55-AM.png)](https://postimg.cc/18LFjZqR)

**In Azure ML Studio:**

- Create Datastore
```python
from azureml.core import Workspace, Datastore
ws = Workspace.get(name='DP100Testing', subscription_id='5fb9753f-1afe-4b8e-a65c-e402ecd9f0a5', resource_group='AZ800')
blob_ds = Datastore.register_azure_blob_container(
    workspace = WS, datastore_name='example',
    container_name='azureml',
    account_name='dp100testing1952224325',
    account_key='lxt3bpnDPOj zcN1081Pg38wMXI0QzRYlbbZnvghwGd82A518uj 5NTZGWZr7F/120jZvVuVGQ4XSk+ASt91vKog=='
)
```

- Print Datastore 
```python
for ds_name in ws.datastores:
    print(ds_name)
```

- Create dataset
```python
VERSION = "1"
my_data = Data(
    path = https://dp100testing1952224325.blob.core.windows.net/azureml/AdventureWorks Sales-2.xlsx,
    type = AssetTypes.URI_FILE,
    description = "Sales",
    name = "AW-Sales",
    version = VERSION
)
ml_client.data.create_or_update(my_data)
```

> ### Preprocessing of Data 

**Steps** for Preprocessing of Data:
- **Data quality assessment**
- **Data Cleaning:** Look for Missing data, Noisy data
- **Data transformation:** Aggregation, Feature selection, Normalization (combining all the data so that everything is standardized)
- **Data reduction:** employ math to filter out unnecessary data

> ### Feature Selection and Feature Engineering

- **Feature Selection:** The process of **selecting specific variables (features)** that contribute the most to the prediction variable in our algorithms.

- **Feature Engineering:** The process of **selecting and expressing data** in a way that improves the performance of machine learning models. 
    - **Wrapper method:** Wrapper methods **evaluate subsets of features****** by training a model on them and assessing the model's performance. All **combinations** are evaluated and the **best one** is chosen. Prone to overfitting
    - **Filter method:** Filter methods **apply statistical techniques** to evaluate the relevance of each feature independently from the machine learning model. They **assign a score to each feature** based on various statistical tests and **rank them accordingly**. Thus **Features** can then be **included or removed** from the dataset **based on their scores**. This method is **relationship based**. Faster than wrapper method. 

> ### Differential Privacy: Eg of Responsible AI

**Differential Privacy** involves mathematically **adding noise to data to protect individual privacy**. It ensures that the **output** of a data analysis algorithm does **not reveal sensitive information** about any individual.
- When applying differential privacy, **Epsilon (ε), Bounds and Sample size**** needs to be **defined**
- **Epsilon (ε)** is a key parameter in differential privacy that **controls the balance between privacy and accuracy:
  - **Low Epsilon (ε)**: High privacy - More noise - High data obscurity - Data more difficult to interpret
  - **High Epsilon (ε)**: Low Privacy - Less noise - Less data obscurity - Data more accurate and easier to interpret 

```python
# Sample code snippet
privacy_usage = { 'epsilon': 0.10},
                data_lower = lower_range[1],
                data_upper = upper_range[10],
                data_rows = sample
```

> ### Accessing Data During Interactive Development

**Data Wrangling:** It is the process of **transforming data** to a format that’s best suited to the needs of ML model. 

- Step 1: Open a notebook with a running ML Azure Kernel

- Step 2: Create **ML Client** and a **datastore** (code mentioned before)

- Step 3: Build a **URI** (**Uniform Resource Identifier**: It specifies location of a resource) OR **Directly grab** it in the **Studio UI**

```python
# Azure Machine Learning workspace details:
subscription = '<subscription_id>'
resource_group = '<resource_group>'
workspace = '<workspace>'
datastore_name = '<datastore>'
path_on_datastore = '<path>'

# Long-form Datastore URI format:
uri = f'azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path_on_datastore}'
```

OR go to Data -> specific Datastore -> select csv file -> copy URI (Datastore URI or Storage URI)
  - **Note:** The **Datastore URI** is only **applicable to Azure ML** and the **Storage URI** is a more generic storage endpoint, which is used only **outside Azure ML**.

- Step 4: Load a **Pandas Dataframe**

```python
# Import pandas library
import pandas as pd

# Populate dataframe "my_dataframe" using the pandas read CSV method by passing in the URI acquired
my_dataframe = pd.read_csv("URI")

# Then run dataframe head passing in a value for the number of rows you want to return
my_dataframe.head(1000)
```

- Step 5: **Wrangle** - Replace **Missing** Strings

```python
# Fill missing values in the "Claim Network Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Claim Network Status": "Unkown"}, inplace=True)

# Fill missing values in the "Payment Status" column with "Unkown" and update the dataframe in place
my_dataframe.fillna(
    value={"Payment Status": "Unkown"}, inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 6: **Wrangle** - **Delete Rows** With any **Empty** Columns

```python
# Drop rows with any missing values and update the dataframe in place
my_dataframe.dropna(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

> ### Wrangling Data with Apache Spark

- Step 1: Create a **compute** to power the notebook - Compute instance / Synapse Spark Pool / Azure ML Serverless Spark

- Step 2: Open a **notebook** and we use **Azure ML Serverless Spark** as compute for ease of use

- Step 3: Build a **URI** or grab it from the Studio UI

- Step 4: Load a **PySpark Pandas Dataframe**

```python
# Import pyspark pandas library
import pyspark.pandas as pd
my_dataframe = pd.read_csv("URI")
my_dataframe.head(1000)
```

- Step 5: Wrangle - Replace **Missing** Strings (code given above)

- Step 6: Wrangle - **Delete rows** with any **empty** columns (code given above)

- Step 7: **Wrangle** - Remove **Duplicate** Rows 

```python
# Drop duplicate rows and update the dataframe in place
my_dataframe.drop_duplicates(inplace=True)

# Sort the dataframe by its index and update the dataframe in place
my_dataframe.sort_index(inplace=True)

# Return the first 1000 rows of the dataframe
my_dataframe.head(1000)
```

- Step 8: **Save** the Transformed Data

```python
# Save the dataframe to a CSV file at the specified Azure Data Lake Storage path
my_dataframe.to_csv(
    "abfss://<FILE_SYSTEM_NAME>@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/data/wrangled"
    )
```

> ### <a href="/posts_blogs/blogs_dp100/Lab2WrangleData" style="color:skyblue;" rel="noopener">Wrangle Data with Python in Azure ML - Lab 2</a>












